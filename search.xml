<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[paper of pruning]]></title>
    <url>%2F2020%2F10%2F10%2Fpaper-of-pruning%2F</url>
    <content type="text"><![CDATA[ADMM ADMM-Pruning A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers. 2018 Yanzi Wang 使用ADMM来进行pruning，效果很好。 SS-Auto: A Single-Shot, Automatic Structured Weight Pruning Framework of DNNs with Ultra-High Efficiency. 2020 ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Method of Multipliers LookAhead: a far-sighted alternative of magnitude-based pruning. PyToch ICLR2020 Weight pruning via adaptive sparsity loss. $L(\{W_i\}, \{b_i\}) + \gamma L_s(\{b_i\})$， 前半部分，来让权重更新，并尽量让稀疏度降低；后半部分，尽量让稀疏度提高；通过两者的组合，来形成一个自适应的调节稀疏度的过程。PyTorch Other DeepTwist : learning model compression via occasional weight distortion. ICLR 2019 reject现在看起来还有点迷惑。与pruning, quantization, svd结合，优化compression的。只需要增加一个超参数，weight distortion 就是 weight-noise-injection]]></content>
      <tags>
        <tag>paper</tag>
        <tag>compression, pruning, sparsity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper-of-quantization]]></title>
    <url>%2F2020%2F09%2F10%2Fpaper-of-quantization%2F</url>
    <content type="text"><![CDATA[PTQvsTAQc1. Post-Training Quantization(PTQ) ZeroQ, EasyQuant, LAPQ, ACIQ,DFQ c2. Training-Aware Quantization(TAQ) S&amp;Q3. Binary, Ternary, 3-4-5-bit, Flexiblec1. Binary c2. Ternary c3. 3-4-5-bit c4. Flexible 2020ECCV2020 PWLQ: Post-Training Piecewise Linear Quantization for Deep Neural Networks. Samsung CVPR2020 ZeroQ: A Novel Zero-Shot Quantization Framework. Berkeley, Peking University AdaBits: Neural Network Quantization with Adaptive Bit-Widths. ByteDance direct adaption, progressive training and joint training 三种方法来量化模型 BiDet: An Efficient Binarized Object Detector. CVPR2020 APQ: Joint Search for Network Architecture, Pruning and Quantization Policy. CVPR2020 MIT architecture, channel pruning, HAQ三者结合的NAS方法，可操作性确实更好，motivation也合适。 IR-Net: Forward and Backward Information Retention for Accurate Binary Neural Networks. CVPR2020 ICLR2020 LSQ: Learned Step Size Quantization ICLR2020 Mixed Precision DNNs: All you need is a good parametrization ICLR2020 sony SAT: Rethinking neural network quantization. Scale-Adjusted Training ICLR2020 reject paper LLSQ: Learned Symmetric Quantization of Neural Networks for Low-precision Integer Hardware. ICLR2020 ICT HAWQv2: Hessian Aware trace-Weighted Quantization of Neural Networks SS-Auto: A Single-Shot, Automatic Structured Weight Pruning Framework of DNNs with Ultra-High Efficiency. IBM Watson Lab BBG: Balanced Binary Neural Networks with Gated Residual. EasyQuant: Post-training Quantization via Scale Optimization. 优化每层的输出的与余弦距离 code中给出了加速效果实验 LAPQ: Loss Aware Post-training Quantization code intel AIPG ACIQ的进化版，方法更加简洁，实现更直接，效果也更加好 2019ICLR2019 ACIQ(pre): analytical clipping for integer quantization of neural networks. ICLR2019 reject intel AIPG Per-Tensor Fixed-point quantization of the back-propagation algorithm. ICLR2019 RQ: Relaxed Quantization for discretized NNs. ICLR2019 NIPS2019 ACIQ Post training 4-bit quantization of convolution networks for rapid-deployment. NIPS 2019 AIPG, Intel ICCV2019 DSQ: Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks. ICCV2019 SenseTime, Beihang DFQ: Data-Free Quantization through Weight Equalization and Bias Correction. Qualcomm 高通 CVPR FQN: Fully Quantized Network for Object Detection. CVPR2019 QIL: Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss CVPR2019 Other SAWB: Accurate and efficient 2-bit quantized neural networks. sysml2019 SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks. 2019 AIPG, Intel Distributed Low Precision Training Without Mixed Precision. Oxford snowcloud.ai Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers. ICT Cambricon int8 for weights and activations, int16 for most of the gradients. 通过量化加快训练过程 WAGEUBN: Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers. 应该算是WAGE的进阶版了 2018ICLR2018 VNQ: Variational network quantization. ICLR2018 WAGE: Training and Inference with Integers in Deep Neural Networks. ICLR2018 oral tsinghua 不仅量化了weight,activation还量化了error, gradient. Alternating multi-bit quantization for recurrent neural networks. ICLR2018 alibaba Mixed Precision Training. FP16 training ICLR2018 baidu Model Compression via distillation and quantization. ICLR2018 google Quantized back-propagation: training binarized neural networks with quantized gradients. ICLR2018 CVPR2018 Clip-Q: Deep network compression learning by In-Parallel Pruning Quantization. CVPR2018 SFU quantization 与pruning同时进行，达到更优的压缩结果。先使用贝叶斯优化搜索 layer-wise的（p,q),然后在进行fine-turning，达到最大目的的压缩权重，没有涉及到激活值的量化和压缩 ELQ: Explicit loss-error-aware quantization for low-bit deep neural networks. CVPR2018 intel tsinghua Quantization and training of neural networks for efficient integer-arithmetic-only inference. CVPR2018 Google TSQ: two-step quantization for low-bit neural networks. CVPR2018 SYQ: learning symmetric quantization for efficient deep neural networks. CVPR2018 xilinx Towards Effective Low-bitwidth Convolutional Neural Networks. CVPR2018 ECCV2018 LQ-NETs: learned quantization for highly accurate and compact deep neural networks. ECCV2018 Microsoft Bi-Real Net: Enhancing the performance of 1-bit CNNs with improved Representational capability and advanced training algorithm. ECCV2018 HKU V-Quant: Value-aware quantization for training and inference of neural networks. ECCV2018 facebook NIPS2018 Heterogeneous Bitwidth Binarization in Convolutional Neural Networks. NIPS2018 microsoft HAQ: Hardware-Aware automated quantization. NIPS workshop 2018 mit Scalable methods for 8-bits training of neural networks. NIPS2018 intel AAAI2018 From Hashing to CNNs: training Binary weights vis hashing. AAAI2018 nlpr Other Synergy: Algorithm-hardware co-design for convnet accelerators on embedded FPGAs. 2018 UC Berkeley Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware. 2018 HALP: High-Accuracy Low-Precision Training. 2018 stanford PACT: parameterized clipping activation for quantized neural networks. 2018 IBM QUENN: Quantization engine for low-power neural networks. CF18ACM UNIQ: Uniform noise injection for non-uniform quantization of neural networks. 2018 Training competitive binary neural networks from scratch. 2018 A white-paper: Quantizing deep convolutional networks for efficient inference. 2018 google 2015-2016 Deep learning with limited numerical precision. 2015 IBM DoReFa-Net: Training low bit-width convolutional neural networks with low bit-width gradients. 2016 BNN: Binarized Neural Networks. NIPS2016 TWNs: Ternary weight networks. NIPS2016 ucas XNOR-Net: ImageNet Classification using binary convolutional neural networks. ECCV2016 washington Hardware-oriented approximation of convolutional neural networks. ICLR2016 Quantized convolutional neural networks for mobile devices. CVPR2016 nlpr 2017 Flexpoint: an adaptive numerical format for efficient training of deep neural networks. 2017 intel INQ: Incremental network quantization, towards lossless CNNs with low-precision weights. ICLR2017 intel labs china TTQ: Trained ternary quantization. ICLR2017 stanford WRPN: wide reduced-precision networks. 2017 Accelerator Architecture Lab, Intel HWGQ: Deep Learning with Low Precision by Half-wave Gaussian Quantization. CVPR2017 A Survey of Model Compression and Acceleration for Deep Neural Networks. 2017 LP-SGD Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent ISCA2017 How to Train a Compact Binary Neural Network with High Accuracy? NLPR MicroSoft Other Fixed point quantization of deep convolutional networks. 2016 Training a binary weight object detector by knowledge transfer for autonomous driving. 2018 Low-bit Quantization of Neural Networks for Efﬁcient Inference. 2019 huawei top]]></content>
      <tags>
        <tag>paper</tag>
        <tag>quantization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Leakage from Gradients]]></title>
    <url>%2F2020%2F01%2F16%2FDeep-Leakage-from-Gradients%2F</url>
    <content type="text"><![CDATA[Motivation在分布式多节点训练时，模型是共享的，梯度由每个节点根据私有的数据训练产生，然后参数服务器根据梯度来更新模型。 共享部分：模型 （包括网络结构和权重） 私有部分：每个子节点的训练数据 可获取：有私有训练数据产生的梯度 DLG的原理：通过梯度来获取私有的训练数据 个人理解总结 简单函数完全可逆假设简单函数如下，简单函数的要求是可以写出其导数的解析表达式$$y = wx^2$$根据BP算法，可以得出权重的梯度：$$\nabla w = \nabla y\cdot 2x \\\nabla y = y - label \\$$ 复杂模型宏观可逆通过DLG可以Given learning model F() and its weights W, if we have the $\nabla w$Obtain the training data Method方法也非常的简单，根据图示，根据随机的输入和label，便可以得到梯度$\nabla W’$，优化$||\nabla W - \nabla w’||^2$就可以了。优化时，将input和label看做可更新的变量，它们的梯度通过BP链反传得到。经过一定的迭代次数后，便能得到高度相似的输入 可以粗略的总结为，$\nabla W$与 $x$之间时双射关系，可以根据其中一个，得到对应的另一个。 batch 训练引出的问题在实际训练中，通常是一个batch来产生一组梯度，假设 batch size = N，那么就有$N!$组合，这些组合产生的梯度是相同的，映射关系如下图所示 在这种条件下，如果同时调节batch的输入，那么每个instance的学习方向往往是不能确定的。paper中也给了解决方案，就是每次只更新batch中的一个instance，这样最终把整个batch更新完。 需要注意的是红框标记部分，DLG只学到了batch组合中的一种情况，和原始图片的顺序不相同，这也和前文的分析一致。 Defense Strategies Noisy Gradients (复杂模型宏观可逆) Gradient Compression and Sparsiﬁcation (复杂模型宏观可逆) Large Batch, High Resolution and Cryptology DLG currently only works for batch size up to 8 and image resolution up to 64×64.]]></content>
      <tags>
        <tag>Attack</tag>
        <tag>distributed training</tag>
        <tag>collaborative learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inspired]]></title>
    <url>%2F2019%2F06%2F14%2FInspired%2F</url>
    <content type="text"><![CDATA[Not everybody could sail the ocean, but still we could share this dream. Caring created resilience. Our longings and our worries are both to some degree overblown because we have within us the capacity to manufacture the very commodity we are always chasing when we choose to experience.]]></content>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOsConfig]]></title>
    <url>%2F2019%2F06%2F13%2FMacOsConfig%2F</url>
    <content type="text"><![CDATA[截图工具Snipast，F1截图，直接可以编辑，复制以及保存。 论文阅读，pdf阅读工具PDF Expert，和iPad可以同步，文档注释功能很全，而且还可以直接进行PDF编辑。 论文语法检查，自动修改工具Grammarly，淘宝上可以买个专业版的账号。 Markdown 记录工具Ulysses 绘图工具OmniGraffle，可以配合LaTeXiT输入公式 思维导图工具MindNode 下载加速工具bnd2 网页浏览ChromeExtensions SwitchyOmega Grammarly OctoLinker Octotree AdBlock uBlock Origin G~F~W SS-NG Linodes ipv6-tunnelssh -N -f -L 8388:[2400:8901::f03c:91ff:****:****]:8388 ****@159.***.**.*** -p **** 邮件浏览网易邮件大师 编程相关Terminal最终感觉还是自带的比较方便，然后换了个背景图片 这也太黑了吧 Sublime Text换了个图标和Skim结合，编译可视化LaTeX Packages Caffe Prototxt Syntax CUDA C++ Julia Latex-cwl LatexTools Package Control Protobuf Syntal Hightlighting Protocol Buffer Syntax SFTP]]></content>
      <tags>
        <tag>MacOS</tag>
        <tag>Software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transposed Convolution in PyTorch]]></title>
    <url>%2F2019%2F05%2F21%2FTransposed-Convolution-in-PyTorch%2F</url>
    <content type="text"><![CDATA[引言Convolution运算的一些参数我们结合图示比较容易理解，卷积核在feature map中滑动，对应元素乘加得到输出。但是Transposed Convolution具体是怎么运算的，其中的参数配置又有什么具体的作用，需要我们初学者深入研究一下。 如图所示，对于Convolution来说，下层为输入，上层为输出；对于Transposed Convolution来说，为Convolution的逆运算或者求导运算，上层为输入，下层为输出。 Transposed Convolution in PyTorch参数列表1torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode=&apos;zeros&apos;) 与公式中变量的对应关系为： Param Variable name in_channels $c_{in}$ out_channels $c_{out}$ kernel_size $k$ stride $s$ padding $p$ output_padding $op$ groups $g$ dilation $d$ $$L_{out} = (L_{in} - 1) * s - 2 \times p + d \times (k - 1) + op + 1$$ 123456789101112131415In [14]: deconvOut[14]: ConvTranspose1d(1, 1, kernel_size=(3,), stride=(1,), bias=False)In [15]: xOut[15]: tensor([[[1., 1.]]])In [16]: deconv.weightOut[16]: Parameter containing:tensor([[[1., 1., 1.]]], requires_grad=True)In [17]: y = deconv(x)In [18]: yOut[18]: tensor([[[1., 2., 2., 1.]]], grad_fn=&lt;SqueezeBackward1&gt;) 123456789101112131415In [22]: xOut[22]: tensor([[[1., 1.]]])In [23]: deconvOut[23]: ConvTranspose1d(1, 1, kernel_size=(3,), stride=(2,), bias=False)In [24]: deconv.weightOut[24]: Parameter containing:tensor([[[1., 1., 1.]]], requires_grad=True)In [25]: y = deconv(x)In [26]: yOut[26]: tensor([[[1., 1., 2., 1., 1.]]], grad_fn=&lt;SqueezeBackward1&gt;) 12345678910111213In [30]: xOut[30]: tensor([[[1., 1.]]])In [31]: deconvOut[31]: ConvTranspose1d(1, 1, kernel_size=(3,), stride=(4,), bias=False)In [32]: deconv.weight.dataOut[32]: tensor([[[1., 1., 1.]]])In [33]: y = deconv(x)In [34]: yOut[34]: tensor([[[1., 1., 1., 0., 1., 1., 1.]]], grad_fn=&lt;SqueezeBackward1&gt;) Convert Transposed Convolution to Convolution]]></content>
      <tags>
        <tag>PyTorch</tag>
        <tag>Transposed Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu environment configuration]]></title>
    <url>%2F2019%2F05%2F02%2Fubuntu-environment-configuration%2F</url>
    <content type="text"><![CDATA[Update /etc/apt/source.list12345678910111213141516171819sudo vim /etc/apt/source.list# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable# deb-src [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable# deb-src [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable# deb-src [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable Base software12345678sudo apt-get install vimsudo apt-get install screensudo apt-get install gitsudo apt-get install zshsh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)"sudo apt-get install htopsudo apt-get install graphvizsudo apt-get install unrar CUDA9.01234567891011121314cd /home/ict/Downloads/cuda9# nvidia-driversudo dpkg -i nvidia-driver-local-repo-ubuntu1604-387.34_1.0-1_amd64.debsudo apt-get updatesudo apt-get install cuda-driverssudo reboot# cuda9.0cd /home/ict/Downloads/cuda9sudo dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-debsudo apt-get updatesudo apt-get install cuda# cudnnsudo dpkg -i libcudnn7-dev_7.1.3.16-1+cuda9.0_amd64.debsudo dpkg -i libcudnn7_7.1.3.16-1+cuda9.0_amd64.deb Remove CUDA12sudo apt-get --purge remove "cuda-*"sudo apt-get --purge remove "nvidia-*" Using the specified GPU1CUDA_VISIBLE_DEVICES="0,1"]]></content>
      <tags>
        <tag>cuda</tag>
        <tag>nvidia-driver</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper note: Tensorflow Lite: Speeding up mobile inference with low precision]]></title>
    <url>%2F2019%2F04%2F12%2Fpaper-note-Tensorflow-Lite-Speeding-up-mobile-inference-with-low-precision%2F</url>
    <content type="text"><![CDATA[Tensorflow Lite: Speeding up mobile inference with low precisionQuantization scheme$$r = S(q-Z)$$ where constants $S$ and $Z$ are quantization parameters, intergers $q$ are mapped to real numbers $r$. param type S Scale fp32 q quantize uint8(w and a) int32(b) Z Zero-point uint8(w and a) 0(b) Note: 怎么表示负数?通过设置合适的$Z$来表示负数 Integer-arithmetic-only matrix multiplication我们考虑weight矩阵的第i行，与activation矩阵的第k列相乘，如下图所示 直接计算的话，需要将uint8扩展为int16，进行乘法运算 将原来的运算展开后，便可以使用uint8的乘法进行运算 Learning quantization parameter以量化到uint8举例，$2^8=256$一共能表示256个大小不同的数，表示的范围为$[a, b]$，先定义如下函数：由于我们采用uint8，所以$n=2^8=256$，所以只需要确定$[a, b]$就可以确定量化的参数$S, Z$$$ S = \frac{b-a}{2^8} \\ Z = \frac{2^8a}{a-b}$$ weight对于w来说，只是简单求min和max得到$[a, b]$ activation对于a来说，在traning时使用exponential moving average(EMA)的方式获取$[a, b]$ WorkflowNote:这个量化流程中，需要进行重新训练，不过按照上边的道理分析，对于8bits不进行训练最后的结果应该也不会太差，对于更低bit的量化，重新训练则是十分重要的。 Experiments主要是accuracy 和latency之间的tradeoff，在ARM cpu上进行latency测试说句实在话，8bits量化的效果不怎么好呀，而且是uint8，再加上一个$Z$,相当于9bit了，但是结果accuracy下降了好几个点。下面是Qcode方式量化的结果，两个网络结构不同，还没来得及做更多的实验，但可以对比一下: Comparison with Qcode method Features Google8bits Qcode 8bits scheme $r = S(q-z)$ type(q)=uint8 $r = 2^nq$ type(q)=int8 quantized training True False BN fusion True True ARM cpu latency experiment True False More experiment(face detection) True TODO Hardware friendly False True References Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference]]></content>
      <tags>
        <tag>paper note</tag>
        <tag>TensorFlow</tag>
        <tag>Quantization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BN_Fusion]]></title>
    <url>%2F2019%2F02%2F26%2FBN-Fusion%2F</url>
    <content type="text"><![CDATA[引言在PyTorch中，我们常在网络中遇到BN层，基本单元如果如下所示，则可以离线将Conv和BN进行fusion，从而在inference时不必计算BN。 在PyTorch中BatchNorm2d的定义如下：1class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 参数列表如下：1dict_keys(['weight', 'bias', 'running_mean', 'running_var', 'num_batches_tracked']) 与公式中变量的对应关系为： Param Variable name eps $\epsilon$ weight $\gamma$ bias $\beta$ running_mean $\mu$ running_var $\sigma^2$ Key IdeaConvolution layer:$$ Y = X * w + b$$BatchNorm layer:$$ Y = \frac{X - \mu}{\sqrt{\sigma^2 + \epsilon}} \gamma + \beta$$ Convolution and BatchNorm fusion: $$ Y = \frac{(X * w + b) - \mu}{\sqrt{\sigma^2 + \epsilon}} \gamma + \beta \\ Y = \frac{\gamma w}{\sqrt{\sigma^2 + \epsilon}}X + \frac{b - \mu}{\sqrt{\sigma^2 + \epsilon}}\gamma + \beta \\ w_{merged} = aw; \quad b_{merged} = (b - \mu)a + \beta; \quad a = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}}$$ Uniform Quantization$$r = Sq$$ where constants $S$ is quantization parameter, intergers $q$ are mapped to real numbers $r$. param type S Scale fp32 q quantize int4 for w int5 for a Quantized convolution layer:$$ Y = X * Sw_q + b \\ where: w = Sw_q$$ Quantized convolution and BatchNorm fusion: $$ Y = \frac{(X * Sw_q + b) - \mu}{\sqrt{\sigma^2 + \epsilon}} \gamma + \beta \\ Y = \frac{\gamma Sw_q}{\sqrt{\sigma^2 + \epsilon}}X + \frac{b - \mu}{\sqrt{\sigma^2 + \epsilon}}\gamma + \beta \\ S_{merged} = aS; \quad w_{merged} = aw; \quad b_{merged} = (b - \mu)a + \beta; \quad a = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}}$$ References: Docs: torch.nn.BatchNorm2d BN层合并原理及实现]]></content>
      <tags>
        <tag>PyTorch</tag>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[suvery-of-qcnn]]></title>
    <url>%2F2019%2F01%2F16%2Fsuvery-of-qcnn%2F</url>
    <content type="text"><![CDATA[Quantization AlgorithmsTruncation without re-training原理与分析将fp32的数，直接截断为8bit定点数以8bits为例，每层的权重共享一个QCode，每层的activation共享一个QCode。$$ x_q = \frac{clip(round(x_f\times 2^{qcode}), -2^7, 2^7-1)}{2^{qcode}}$$ 如何配置QCode accuracy-aware config对于分类网络来说，我们只关心它最后的分类精度，比如baseline网络在ImageNet上分类精度为70%，通过改变配置config，能够得到最后量化后的网络的分类精度。配置Qcode的指标只关心最后的分类精度。 Greedy layer-wise optimization另外，可以采用更加方便的方式，通过最小化$x_q$与$x$之间的KL散度或者MSE，来配置每层的QCode. 特点分析 运算简单，不需要重新进行训练 需要逐层配置QCode参数，耗费较多资源 QCode的量化编码方式过于简单，且Qcode只能取值为整数，对分类网络比较友好，但是对检测网络和其他类型的网络并不一定能轻松适配。 试验与结论使用这种方式进行离线量化，最后的分类网络的精度基本不会下降（千分位的变动）. DoReFa NetFor weights:$$ w^k = 2 quantize_k(\frac{\tanh(w_i)}{2 \max(|\tanh(w_i)|)} + \frac{1}{2}) - 1$$uniform quantization :( 为什么不直接使用clip函数，明明最后用的是[-1, 1]之间的数值，$tanh$的作用感觉也没有那么大；在某些已经训练好的网络中，weight的分布可能是非常集中在0附近的，如果仍然使用这个映射方式，就导致该层的weight分布强行被拉到[-1, 1]之间，那该层的功能性是不是就改变了？这是不是 DoReFa类型的量化对 first and last layer 敏感的原因呢？For activations:$$ a^k = quantize_k(clip(r, 0, 1))$$uniform quantization :( Towards Effective Low-bitwidth Convolutional Neural NetworksThe quantization equation:$$ z_q = Q(z_r) = \frac{1}{2^k - 1}round((2^k - 1)z_r)$$where $z_r \in [0, 1]$ denotes the full-precision value and $z_q \in [0, 1]$denotes the quantized value. For weights:$$ w_q = Q(\frac{\tanh(w)}{2\max(|\tanh(w)|)} + \frac{1}{2})$$应该是paper中写错了，最后应该和DoFeFa的量化方式一样For activation:$$ x_q = Q(clip(x, 0, 1))$$Paper 主要做了一下几组实验：TS Two step: quantize weight =&gt; quantize activationPQ Progressive quantization: quantization from higher precisions to lower precisionsGuided Teacher student training. WRPN受到了Wide ResNet的启发，Wide ResNet减少网络的深度，扩展网络的宽度，通过重新设计了网络结构的方式来使得网络保持原有的精度。WRPN wide reduced-precision networks需要对weights和activation进行量化，相当于减少了网络的拟合能力，可以通过增加网络宽度的方式来增加一部分参数来弥补量化带来的损失。 半成品的实现：Code 原理分析 对w进行[-1, 1]截断，对a进行[0, 1]截断，截断使用clip_by_value函数； 对w进行有符号量化，对a进行无符号量化$$ w_q = \frac{1}{2^{k-1}-1}round((2^{k-1}-1)\cdot w_f) \\ a_q = \frac{1}{2^{k}-1}round((2^{k}-1)\cdot a_f)$$ 特点分析 quantier的parameter是静态的，不需要像TTQ一样需要在训练中进行学习 In our work, we maintain the depth parameter same as baseline network but widen the filter maps. To be consistent with results reported in prior works, we do not quantize weights and activations of the first and last layer. 最后是在Intel Arria 10 进行了FPGA的实现，Exploration of Low Numeric Precision Deep Learning Inference Using Intel ® FPGAs论文中又有了更加详细的实现细节。 试验与结论 VNQ: Variational Network QuantizationOur method is an extension of Sparse VD 公式很多，还需要时间推导。 原理分析特点分析 The method does not require fine-tuning after quantization.可以从scratch开始训练，也可以直接使用预训练好的权重。 Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).只有小数据集小网络，小数据集大网络，没有大数据集和大网络，可能不具有普适性。 WAGE: Training and Inference with Integers in Deep Neural Networks官方源码：code,只包含cifar10的demo。将Weight,Activation,Gradient,Error都进行量化，量化方式采用$$ \sigma(k) = 2^{k-1}, k\in \mathbb{N_+} \\ Q(x, k) = clip(\sigma(k)\cdot round(\frac{x}{\sigma(k)}), -1+\sigma(k), 1-\sigma(k))$$ 试验与结论如果需要在AI芯片进行训练，WAGE是一个很好的研究方向；但是当前主流的做法是训练交给GPU，只需要在AI芯片上进行inference. Clip-Q: Deep network compression learning by In-Parallel Pruning Quantization We combine network pruning and weight quantization in a single learning framework 使用贝叶斯优化器来the pruning rate p and the bit budget b 只对weights进行了pruning和quantization. pruning是百分比的方式，quantization是聚类codebook的方式 试验与结论量化相关试验我认为还是比较相同bitwidth下的精度更切合实际应用，毕竟量化的作用一方面是压缩权重，更重要的还是AI芯片的inference加速，需要给出hardware friendly的分析与验证。 Bi-Real Net 较XOR-Net, BNN网络的分类accuracy有较大的提升 没有量化第一层和最后一层，并不Real:) 试验与结论 Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware ELQ: Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks 之前的方式大多是layer-wise 最小化量化前后数值的 Error，本文建立了quantization与loss之间的关系。其实在pruning中也有类似的改进，一般大家的认为weights越小越不重要，优先prune掉，后来提出fisher pruning就是根据loss来确定pruning的策略。前者简单直接，后者理论上能找到更优的解，但实现麻烦。 只量化了weights PACT: parameterized clipping activation for quantized neural networks PACT提出Activation的量化方法，并使用DoReFa量化weight，最终训练w4a4 网络在ImageNet上仍能达到很好的accuracy，编码方式也十分简单，很值得借鉴。 仍然没有量化第一层和最后一层 原理分析 Forward quantization$$ y = PACT(x) = 0.5(|x|-|x-\alpha|+\alpha) \\ y = round(y\cdot \frac{2^k-1}{\alpha})\cdot \frac{\alpha}{2^k-1}$$这里也看出k对应的是unsigned int的位数。在ResNet的bottle net中，含有没有ReLU的feature map, 在一些比较常用的物体检测网络比如YOLO中，采用了leacky ReLU, 在PACT中，并没有对其进行特殊的考虑(它只考虑了无符号的情况)。 Backward, STE:$$ \cfrac{\partial y_q}{\partial\alpha} = \cfrac{\partial y_q}{\partial y}\cfrac{\partial y}{\partial\alpha}\simeq \cfrac{\partial y}{\partial \alpha}$$ 试验与结论weights使用DoReFa方式量化，activations使用PACT方式量化，可以看到在ResNet18上w4a4只和baseline低了一个点。 HWGQ: Deep Learning with Low Precision by Half-wave Gaussian Quantization. CVPR2017主要是w1a2 FQN: Fully Quantized Network for Object Detection. CVPR2019主要在物体检测网络上进行了w4a4的量化实验，效果比较好 $$ X^Q = Q_k(X^R)\in\{q_0, q_1, q_2, \dots, q_{2^k-1}\} \\ X^Q = \Delta(X^I - z) \\ lb = q_0, ub = q_{2^k-1} \\ \Delta = \frac{ub - lb}{2^k-1}$$ For weight:$$ lb = \min(W) \\ ub = \max(W)$$ For activation:$$ lb^l_{EMA} = EMA(\min(A), \gamma=0.999) \\ ub^l_{EMA} = EMA(\max(A), \gamma=0.999)$$为了减少activation的波动性，作者将bn freeze住，并在后续的实验部分验证了做法的有效性。具体做法为，将bn的参数修正到与之对应的conv和fc的weight和bias中，量化时是对修正后的$weight_{merged}$进行量化，训练时更新weight，而不是更新$weight_{merged}$ 如何integer only计算卷积$$ y = Q_k(A^R)\times Q_k(W^R) \\ = \Delta_a(A^I - z_a) \times \Delta_w(W^I - z_w) \\ = \Delta_a \Delta_w (A^I W^I - A^I z_w - W^I z_a + z_a z_w)$$ References: paper-of-quantization nervanasystems.github.io]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>quantization</tag>
        <tag>compression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mxnet-build-from-source]]></title>
    <url>%2F2019%2F01%2F08%2Fmxnet-build-from-source%2F</url>
    <content type="text"><![CDATA[Build MXNet from SourceClone the MXNet Project12git clone --recursive https://github.com/apache/incubator-mxnet mxnetcd mxnet Download mklcudnn12cd 3rdparty/mkldnn/externalwget https://github.com/intel/mkl-dnn/releases/download/v0.17.2/mklml_lnx_2019.0.1.20180928.tgz Build12cd docs/install ./install_mxnet_ubuntu_python.sh install python12cd pythonpip install -e . Add operator in backendWhy not add custom operator using PythonOp interface.123456789class NDArrayOp(PythonOp): """Base class for numpy operators. numpy operators allow parts of computation in symbolic graph to be writen in numpy. This feature is intended for quickly hacking out a solution for non performance critical parts. Please consider write a c++ implementation if it becomes a bottleneck. Note that if your operator contains internal states (like arrays), it cannot be used for multi-gpu training. """ https://cwiki.apache.org/confluence/display/MXNET/CLion+setup+for+MXNet+development+on+Mac Referencesinstall-mxnet-for-pythonadd op in backend]]></content>
      <tags>
        <tag>MXNet</tag>
        <tag>GLUON</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install-tensorRT]]></title>
    <url>%2F2018%2F10%2F26%2Finstall-tensorRT%2F</url>
    <content type="text"><![CDATA[Install Driver and CUDAinstall cuda Install TensorRTDownload TensorRT 5.0tensorrt download Then, I got the following package:nv-tensorrt-repo-ubuntu1604-cuda9.0-trt5.0.0.10-rc-20180906_1-1_amd64.deb Install TensorRT 5.01234567sudo dpkg -i nv-tensorrt-repo-ubuntu1604-cuda9.0-trt5.0.0.10-rc-20180906_1-1_amd64.debsudo apt-key add /var/nv-tensorrt-repo-cuda9.0-trt5.0.0.10-rc-20180906/7fa2af80.pub sudo apt-get update# Unluckily, I encountered the following problem.## double free or corruption (fasttop): 0x0000000001368e00 ***# I solved it by run: sudo apt-get purge libappstream3 sudo apt-get install tensorrt Demo1234cd /usr/src/tensorrt/samplessudo make -j32cd ../bin./samples_mnist Install PyCUDA12345678pip install pycuda# error# In file included from src/cpp/cuda.cpp:1:0:# src/cpp/cuda.hpp:14:18: fatal error: cuda.h: No such file or directory# compilation terminated.# error: command 'gcc' failed with exit status 1export PATH=/usr/local/cuda/bin:$PATHpip install pycuda uff custom plugin123456cd /usr/src/tensorrt/samples/python/uff_custom_pluginmkdir build &amp;&amp; pushd buildcmake ..make -j8python2 lenet5.pypython2 mnist_uff_custom_plugin.py]]></content>
      <tags>
        <tag>cuda</tag>
        <tag>nvidia-driver</tag>
        <tag>ubuntu</tag>
        <tag>tensorRT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git-command]]></title>
    <url>%2F2018%2F09%2F29%2Fgit-command%2F</url>
    <content type="text"><![CDATA[Git commandupdate a forked repo from remote repo.123456789git remote add upstream git@github.com:&lt;custom&gt;.gitgit remote -vgit fetch upstreamgit merge upstream/mastergit push git 合并多个commits合并多个 Commit git拉取远程分支到本地查看远程分支1git branch -r 拉取远程分支到本地分支1git checkout -b 本地分支名x origin/远程分支名x 取消最近的一次add12git reset &lt;file&gt;git reset 取消本地修改1234git checkout . #本地所有修改的。没有的提交的，都返回到原来的状态git stash #把所有没有提交的修改暂存到stash里面。可用git stash pop回复。git reset --hard HASH #返回到某个节点，不保留修改。git reset --soft HASH #返回到某个节点。保留修改]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[anaconda-config]]></title>
    <url>%2F2018%2F09%2F28%2Fanaconda-config%2F</url>
    <content type="text"><![CDATA[Install anaconda on MacOS清华镜像站 1bash Anaconda3 Install cv21conda install -c menpo opencv]]></content>
      <tags>
        <tag>anaconda python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycaffe config]]></title>
    <url>%2F2018%2F08%2F27%2Fpycaffe-config%2F</url>
    <content type="text"><![CDATA[ProblemThere is always some trouble when we want use pycaffe and opencv at the same time :(12import caffeimport cv2 Solution We just do not use Anaconda!!!!! 123cd caffe/pythonfor req in $(cat requirements.txt); do pip install $req; donepip install opencv-python 12345678910111213141516import os.path as ospimport sysdef add_path(path): if path not in sys.path: sys.path.insert(0, path)caffe_path = '/home/zhaoxiandong/caffe'# Add caffe to PYTHONPATHcaffe_path = osp.join(caffe_path, 'python')add_path(caffe_path)import caffeimport cv2# successful !::::)))) Referenceshttps://github.com/NVIDIA/DIGITS/issues/156]]></content>
      <tags>
        <tag>pycaffe</tag>
        <tag>caffe</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch Begin]]></title>
    <url>%2F2018%2F08%2F10%2FPyTorch-Begin%2F</url>
    <content type="text"><![CDATA[Recommand approach for saving modelStack overflow First 12345# savetorch.save(model.state_dict(), PATH)# loadmodel = Model(args)model.load_state_dict(torch.load(PATH)) Second 1234# savetorch.save(mode, PATH)# loadmodel = torch.load(PATH) Pytorch DataParallelcsdn]]></content>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh tunnel 端口转发]]></title>
    <url>%2F2018%2F08%2F09%2Fssh-tunnel-%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%2F</url>
    <content type="text"><![CDATA[Problem description A PC B 有公网IP的服务器或者工作站 C 和B在同一个局域网的机器 D 任意一台能联网的机器 我们想通过PC来连接B, C, D, 从而方便的来远程同步代码，和开启jupyter-notebook服务等。 ssh command主要用到了下边这条命令： 1ssh -N -f -L &lt;port2&gt;:&lt;ip1&gt;:&lt;port1&gt; &lt;username&gt;@&lt;ip&gt; N 在后台运行 f Fork into background after authentication. 后台认证用户密码，通常和-N连用，不用登录到远程主机。 L 本地起端口映射到其他机器 ExampleAccess server C on PC ARun on PC A:1ssh -N -f -L &lt;A.custom.port&gt;:&lt;C.local.ip&gt;:&lt;C.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt; Access server D on PC ARun on server D:1ssh -CfnNt -R &lt;B.custom.port&gt;:localhost:&lt;D.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt; Run on PC A:1ssh -N -f -L &lt;A.custom.port&gt;:localhost:&lt;B.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt; A直接ssh登陆到CAdd the following code to ~/.ssh/config12345Host serverUser C.usernamePort 22HostName &lt;C.local.ip&gt;ProxyCommand ssh B.username@B.public.ip nc %h %p 2&gt; /dev/null Then, we can connect to server C directly.1ssh server Reference梦溪博客]]></content>
      <tags>
        <tag>ssh tunnel</tag>
        <tag>sublime</tag>
        <tag>sftp</tag>
        <tag>jupyter-notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python argparse]]></title>
    <url>%2F2018%2F08%2F08%2Fpython-argparse%2F</url>
    <content type="text"><![CDATA[Usage1234import argparseparser= argparse.ArgumentParser()parser = argparse.ArgumentParser(description='')parser.add_argument('data', metavar='DIR', help='path to dataset') Parameter prog - The name of the program (default: sys.argv[0]) usage - The string describing the program usage (default: generated from arguments added to parser) description - Text to display before the argument help (default: none) epilog - Text to display after the argument help (default: none) parents - A list of ArgumentParser objects whose arguments should also be included formatter_class - A class for customizing the help output prefix_chars - The set of characters that prefix optional arguments (default: ‘-‘) fromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None) argument_default - The global default value for arguments (default: None) conflict_handler - The strategy for resolving conflicting optionals (usually unnecessary) add_help - Add a -h/–help option to the parser (default: True) allow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True) The add_augment() method name or flags - Either a name or a list of option strings, e.g. foo or -f, –foo. action - The basic type of action to be taken when this argument is encountered at the command line. nargs - The number of command-line arguments that should be consumed. const - A constant value required by some action and nargs selections. default - The value produced if the argument is absent from the command line. type - The type to which the command-line argument should be converted. choices - A container of the allowable values for the argument. required - Whether or not the command-line option may be omitted (optionals only). help - A brief description of what the argument does. metavar - A name for the argument in usage messages. 有助于提醒用户，该命令行参数所期待的参数，如 metavar=”mode” dest - The name of the attribute to be added to the object returned by parse_args(). ReferencePython 命令行解析python.org]]></content>
      <tags>
        <tag>python</tag>
        <tag>argparse</tag>
      </tags>
  </entry>
</search>
