<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>paper-of-quantization</title>
      <link href="/2020/09/10/paper-of-quantization/"/>
      <url>/2020/09/10/paper-of-quantization/</url>
      
        <content type="html"><![CDATA[<h3 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h3><ol><li><strong>LSQ</strong>: Learned Step Size Quantization <strong>ICLR2020</strong></li><li>Mixed Precision DNNs: All you need is a good parametrization <strong>ICLR2020 sony</strong></li><li><strong>HAWQv2</strong>: Hessian Aware trace-Weighted Quantization of Neural Networks</li><li><strong>LLSQ</strong>: Learned Symmetric Quantization of Neural Networks for Low-precision Integer Hardware. <strong>ICLR2020</strong> <strong>ICT</strong></li><li><strong>ZeroQ</strong>: A Novel Zero-Shot Quantization Framework. <strong>Berkeley, Peking University</strong></li><li><strong>SS-Auto</strong>: A Single-Shot, Automatic Structured Weight Pruning Framework of DNNs with Ultra-High Efficiency. <strong>IBM Watson Lab</strong></li><li><strong>IR-Net</strong>: Forward and Backward Information Retention for Accurate Binary Neural Networks. <strong>CVPR2020</strong></li><li><strong>BiDet</strong>: An Efficient Binarized Object Detector. <strong>CVPR2020</strong></li><li><strong>BBG</strong>: Balanced Binary Neural Networks with Gated Residual.</li><li><strong>APQ</strong>: Joint Search for Network Architecture, Pruning and Quantization Policy. <strong>CVPR2020 MIT</strong> architecture, channel pruning, HAQ三者结合的NAS方法，可操作性确实更好，motivation也合适。</li></ol><h3 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h3><h4 id="ICLR"><a href="#ICLR" class="headerlink" title="ICLR"></a>ICLR</h4><ol><li><strong>ACIQ</strong>: analytical clipping for integer quantization of neural networks. <strong>ICLR2019</strong> Intel</li><li>Per-Tensor Fixed-point quantization of the back-propagation algorithm. <strong>ICLR2019</strong></li><li><strong>RQ</strong>: Relaxed Quantization for disretized NNs. <strong>ICLR2019</strong></li></ol><h4 id="NIPS"><a href="#NIPS" class="headerlink" title="NIPS"></a>NIPS</h4><ol><li><strong>Post training</strong> 4-bit quantization of convolution networks for rapid-deployment. <strong>NIPS 2019</strong> AIPG, Intel</li></ol><h4 id="ICCV"><a href="#ICCV" class="headerlink" title="ICCV"></a>ICCV</h4><ol><li><strong>DSQ</strong>: Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks. <strong>ICCV2019</strong> SenseTime, Beihang</li></ol><h4 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h4><ol><li><strong>FQN</strong>: Fully Quantized Network for Object Detection. <strong>CVPR2019</strong></li><li><strong>QIL</strong>: Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss <strong>CVPR2019</strong></li></ol><h4 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h4><ol><li><strong>SAWB</strong>: Accurate and efficient 2-bit quantized neural networks. <strong>sysml2019</strong></li><li><strong>SQuantizer</strong>: Simultaneous Learning for Both Sparse and Low-precision Neural Networks. <strong>2019</strong> AIPG, Intel</li><li>Distributed Low Precision Training Without Mixed Precision. <strong>Oxford snowcloud.ai</strong></li><li><strong>Adaptive</strong> Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers. <strong>ICT Cambricon</strong> int8 for weights and activations, int16 for most of the gradients.</li><li><strong>AdaBits</strong>: Neural Network Quantization with Adaptive Bit-Widths. <strong>ByteDance</strong> 想法和上篇论文很像</li><li><strong>WAGEUBN</strong>: Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers. 应该算是WAGE的进阶版了</li></ol><h3 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h3><h4 id="ICLR2018"><a href="#ICLR2018" class="headerlink" title="ICLR2018"></a>ICLR2018</h4><ol><li><strong>VNQ</strong>: Variational network quantization. <strong>ICLR2018</strong></li><li><strong>WAGE</strong>: Training and Inference with Integers in Deep Neural Networks. <strong>ICLR2018</strong> oral tsinghua 不仅量化了weight,activation还量化了error, gradient.</li><li>Alternating multi-bit quantization for recurrent neural networks. <strong>ICLR2018</strong> alibaba</li><li>Mixed Precision Training. FP16 training <strong>ICLR2018</strong> baidu</li><li>Model Compression via distillation and quantization. <strong>ICLR2018</strong> google</li><li>Quantized back-propagation: training binarized neural networks with quantized gradients. <strong>ICLR2018</strong></li></ol><h4 id="CVPR2018"><a href="#CVPR2018" class="headerlink" title="CVPR2018"></a>CVPR2018</h4><ol><li><strong>Clip-Q</strong>: Deep network compression learning by In-Parallel Pruning Quantization. <strong>CVPR2018</strong> SFU</li><li><strong>ELQ</strong>: Explicit loss-error-aware quantization for low-bit deep neural networks. <strong>CVPR2018</strong> intel tsinghua</li><li>Quantization and training of neural networks for efficient integer-arithmetic-only inference. <strong>CVPR2018</strong> Google</li><li><strong>TSQ</strong>: two-step quantization for low-bit neural networks. <strong>CVPR2018</strong></li><li><strong>SYQ</strong>: learning symmetric quantization for efficient deep neural networks. <strong>CVPR2018</strong> xilinx</li><li>Towards Effective Low-bitwidth Convolutional Neural Networks. <strong>CVPR2018</strong></li></ol><h4 id="ECCV2018"><a href="#ECCV2018" class="headerlink" title="ECCV2018"></a>ECCV2018</h4><ol><li><strong>LQ-NETs</strong>: learned quantization for highly accurate and compact deep neural networks. <strong>ECCV2018</strong> Microsoft</li><li><strong>Bi-Real Net</strong>: Enhancing the performance of 1-bit CNNs with improved Representational capability and advanced training algorithm. <strong>ECCV2018</strong> HKU</li><li><strong>V-Quant</strong>: Value-aware quantization for training and inference of neural networks. <strong>ECCV2018</strong> facebook</li></ol><h4 id="NIPS2018"><a href="#NIPS2018" class="headerlink" title="NIPS2018"></a>NIPS2018</h4><ol><li>Heterogeneous Bitwidth Binarization in Convolutional Neural Networks. <strong>NIPS2018</strong> microsoft</li><li><strong>HAQ</strong>: Hardware-Aware automated quantization. <strong>NIPS workshop 2018</strong> mit</li><li>Scalable methods for 8-bits training of neural networks. <strong>NIPS2018</strong> intel</li></ol><h4 id="AAAI2018"><a href="#AAAI2018" class="headerlink" title="AAAI2018"></a>AAAI2018</h4><ol><li>From Hashing to CNNs: training Binary weights vis hashing. <strong>AAAI2018</strong> nlpr</li></ol><h4 id="Other-1"><a href="#Other-1" class="headerlink" title="Other"></a>Other</h4><ol><li><strong>Synergy</strong>: Algorithm-hardware co-design for convnet accelerators on embedded FPGAs. <strong>2018</strong> UC Berkeley</li><li>Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware. <strong>2018</strong></li><li><strong>HALP</strong>: High-Accuracy Low-Precision Training. <strong>2018</strong> stanford</li><li><strong>PACT</strong>: parameterized clipping activation for quantized neural networks. <strong>2018</strong> IBM</li><li><strong>QUENN</strong>: Quantization engine for low-power neural networks. <strong>CF18ACM</strong></li><li><strong>UNIQ</strong>: Uniform noise injection for non-uniform quantization of neural networks. <strong>2018</strong></li><li>Training competitive binary neural networks from scratch. <strong>2018</strong></li><li><strong>A white-paper</strong>: Quantizing deep convolutional networks for efficient inference. <strong>2018</strong> google</li></ol><h3 id="2015-2016"><a href="#2015-2016" class="headerlink" title="2015-2016"></a>2015-2016</h3><ol><li>Deep learning with limited numerical precision. <strong>2015 IBM</strong></li><li><strong>DoReFa-Net</strong>: Training low bit-width convolutional neural networks with low bit-width gradients. <strong>2016</strong></li><li><strong>BNN</strong>: Binarized Neural Networks. <strong>NIPS2016</strong></li><li><strong>TWNs</strong>: Ternary weight networks. <strong>NIPS2016</strong> ucas</li><li><strong>XNOR-Net</strong>: ImageNet Classification using binary convolutional neural networks. <strong>ECCV2016 washington</strong></li><li>Hardware-oriented approximation of convolutional neural networks. <strong>ICLR2016</strong></li><li>Quantized convolutional neural networks for mobile devices. <strong>CVPR2016</strong> nlpr</li></ol><h3 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h3><ol start="9"><li><strong>Flexpoint</strong>: an adaptive numerical format for efficient training of deep neural networks. <strong>2017</strong> intel</li><li><strong>INQ</strong>: Incremental network quantization, towards lossless CNNs with low-precision weights. <strong>ICLR2017</strong> intel labs china</li><li><strong>TTQ</strong>: Trained ternary quantization. <strong>ICLR2017</strong> stanford</li><li><strong>WRPN</strong>: wide reduced-precision networks. <strong>2017</strong> Accelerator Architecture Lab, Intel</li><li><strong>HWGQ</strong>: Deep Learning with Low Precision by Half-wave Gaussian Quantization. <strong>CVPR2017</strong></li><li>A <strong>Survey</strong> of Model Compression and Acceleration for Deep Neural Networks. <strong>2017</strong></li><li><strong>LP-SGD</strong> Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent <strong>ISCA2017</strong></li><li>How to Train a Compact Binary Neural Network with High Accuracy? <strong>NLPR MicroSoft</strong></li></ol><h2 id="Other-2"><a href="#Other-2" class="headerlink" title="Other"></a>Other</h2><ol><li>Fixed point quantization of deep convolutional networks. <strong>2016</strong></li><li>Training a binary weight object detector by knowledge transfer for autonomous driving. <strong>2018</strong></li><li>Low-bit Quantization of Neural Networks for Efﬁcient Inference. <strong>2019</strong> huawei</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> quantization </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deep Leakage from Gradients</title>
      <link href="/2020/01/16/Deep-Leakage-from-Gradients/"/>
      <url>/2020/01/16/Deep-Leakage-from-Gradients/</url>
      
        <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><img src="dlg_motivation.png" width="70%" height="70%"><br>在分布式多节点训练时，模型是共享的，梯度由每个节点根据私有的数据训练产生，然后参数服务器根据梯度来更新模型。</p><p>共享部分：<br>模型 （包括网络结构和权重）</p><p>私有部分：<br>每个子节点的训练数据</p><p>可获取：<br>有私有训练数据产生的梯度</p><p>DLG的原理：<strong>通过梯度来获取私有的训练数据</strong></p><blockquote><p>个人理解总结</p></blockquote><h2 id="简单函数完全可逆"><a href="#简单函数完全可逆" class="headerlink" title="简单函数完全可逆"></a>简单函数完全可逆</h2><p>假设简单函数如下，简单函数的要求是可以写出其导数的解析表达式<br>$$y = wx^2$$<br>根据BP算法，可以得出权重的梯度：<br>$$<br>\nabla w = \nabla y\cdot 2x \\<br>\nabla y = y - label \\<br>$$</p><h2 id="复杂模型宏观可逆"><a href="#复杂模型宏观可逆" class="headerlink" title="复杂模型宏观可逆"></a>复杂模型宏观可逆</h2><p>通过DLG可以<br>Given learning model F() and its weights W, if we have the $\nabla w$<br>Obtain the training data</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="dlg_method.png" width="70%" height="70%"><br>方法也非常的简单，根据图示，根据随机的输入和label，便可以得到梯度$\nabla W’$，优化$||\nabla W - \nabla w’||^2$就可以了。优化时，将input和label看做可更新的变量，它们的梯度通过BP链反传得到。经过一定的迭代次数后，便能得到高度相似的输入</p><p><img src="out.gif" width="40%" height="40%"><br>可以粗略的总结为，$\nabla W$与 $x$之间时双射关系，可以根据其中一个，得到对应的另一个。<br><img src="dlg_image.png" width="20%" height="20%"></p><h2 id="batch-训练引出的问题"><a href="#batch-训练引出的问题" class="headerlink" title="batch 训练引出的问题"></a>batch 训练引出的问题</h2><p>在实际训练中，通常是一个batch来产生一组梯度，假设 batch size = N，那么就有$N!$组合，这些组合产生的梯度是相同的，映射关系如下图所示</p><p><img src="dlg_batch.png" width="20%" height="20%"><br>在这种条件下，如果同时调节batch的输入，那么每个instance的学习方向往往是不能确定的。paper中也给了解决方案，就是每次只更新batch中的一个instance，这样最终把整个batch更新完。</p><p>需要注意的是红框标记部分，DLG只学到了batch组合中的一种情况，和原始图片的顺序不相同，这也和前文的分析一致。<br><img src="dlg_result.png" width="70%" height="70%"></p><h2 id="Defense-Strategies"><a href="#Defense-Strategies" class="headerlink" title="Defense Strategies"></a>Defense Strategies</h2><ul><li>Noisy Gradients (复杂模型宏观可逆)</li><li>Gradient Compression and Sparsiﬁcation (复杂模型宏观可逆)</li><li>Large Batch, High Resolution and Cryptology<br>  DLG currently only works for batch size up to 8 and image resolution up to 64×64.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Attack </tag>
            
            <tag> distributed training </tag>
            
            <tag> collaborative learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Inspired</title>
      <link href="/2019/06/14/Inspired/"/>
      <url>/2019/06/14/Inspired/</url>
      
        <content type="html"><![CDATA[<ol><li>Not everybody could sail the ocean, but still we could share this dream.</li><li><a href="https://www.ted.com/talks/kelly_mcgonigal_how_to_make_stress_your_friend?rid=1xTPFKlfGU2N" target="_blank" rel="noopener">Caring created resilience.</a></li><li><a href="https://www.ted.com/talks/dan_gilbert_asks_why_are_we_happy/transcript?rid=RZ0QeN6mS9mj" target="_blank" rel="noopener">Our longings and our worries are both to some degree overblown because we have within us the capacity to manufacture the very commodity we are always chasing when we choose to experience.</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> life </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MacOsConfig</title>
      <link href="/2019/06/13/MacOsConfig/"/>
      <url>/2019/06/13/MacOsConfig/</url>
      
        <content type="html"><![CDATA[<h1 id="截图工具"><a href="#截图工具" class="headerlink" title="截图工具"></a>截图工具</h1><p><a href="https://www.snipaste.com/" target="_blank" rel="noopener">Snipast</a>，F1截图，直接可以编辑，复制以及保存。<br><!-- <img src="snipaste_logo.svg" width="5%" height="5%">  --></p><h1 id="论文阅读，pdf阅读工具"><a href="#论文阅读，pdf阅读工具" class="headerlink" title="论文阅读，pdf阅读工具"></a>论文阅读，pdf阅读工具</h1><p><a href="https://pdfexpert.com/" target="_blank" rel="noopener">PDF Expert</a>，和iPad可以同步，文档注释功能很全，而且还可以直接进行PDF编辑。<br><!-- <img src="pdfexpert-fav-96.png" width="5%" height="5%"> --></p><h1 id="论文语法检查，自动修改工具"><a href="#论文语法检查，自动修改工具" class="headerlink" title="论文语法检查，自动修改工具"></a>论文语法检查，自动修改工具</h1><p><a href="https://www.grammarly.com/" target="_blank" rel="noopener">Grammarly</a>，淘宝上可以买个专业版的账号。<br><!-- <img src="grammarly.jpg" width="20%" height="20%"> --></p><h1 id="Markdown-记录工具"><a href="#Markdown-记录工具" class="headerlink" title="Markdown 记录工具"></a>Markdown 记录工具</h1><p><a href="https://ulysses.app/" target="_blank" rel="noopener">Ulysses</a><br><!-- <img src="https://ulysses.app/assets/logos/Ulysses-Mac.png" width="10%" height="10%"> --></p><h1 id="绘图工具"><a href="#绘图工具" class="headerlink" title="绘图工具"></a>绘图工具</h1><p><a href="https://www.omnigroup.com/omnigraffle/" target="_blank" rel="noopener">OmniGraffle</a>，可以配合<a href="https://www.chachatelier.fr/latexit/" target="_blank" rel="noopener">LaTeXiT</a>输入公式<br><!-- <img src="https://www.omnigroup.com/assets/img/icons/omnigraffle-mac@2x.png" width="10%" height="10%"> --></p><h1 id="思维导图工具"><a href="#思维导图工具" class="headerlink" title="思维导图工具"></a>思维导图工具</h1><p><a href="https://mindnode.com/" target="_blank" rel="noopener">MindNode</a><br><!-- <img src="https://mindnode.com/assets/icon.png" width="10%" height="10%"> --></p><h1 id="下载加速工具"><a href="#下载加速工具" class="headerlink" title="下载加速工具"></a>下载加速工具</h1><p><a href="https://github.com/b3log/baidu-netdisk-downloaderx" target="_blank" rel="noopener">bnd2</a></p><h1 id="网页浏览"><a href="#网页浏览" class="headerlink" title="网页浏览"></a>网页浏览</h1><h2 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h2><h3 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h3><ol><li>SwitchyOmega</li><li>Grammarly</li><li>OctoLinker</li><li>Octotree</li><li>AdBlock</li><li>uBlock Origin</li></ol><h2 id="G-F-W"><a href="#G-F-W" class="headerlink" title="G~F~W"></a>G~F~W</h2><ol><li><a href="https://github.com/shadowsocks/ShadowsocksX-NG" target="_blank" rel="noopener">SS-NG</a></li><li><a href="https://cloud.linode.com/linodes" target="_blank" rel="noopener">Linodes</a></li><li>ipv6-tunnel<br><code>ssh -N -f -L 8388:[2400:8901::f03c:91ff:****:****]:8388 ****@159.***.**.*** -p ****</code></li></ol><h1 id="邮件浏览"><a href="#邮件浏览" class="headerlink" title="邮件浏览"></a>邮件浏览</h1><p><a href="https://mail.163.com/dashi/" target="_blank" rel="noopener">网易邮件大师</a></p><h1 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h1><h2 id="Terminal"><a href="#Terminal" class="headerlink" title="Terminal"></a>Terminal</h2><p>最终感觉还是自带的比较方便，然后换了个背景图片<br><img src="astronomy-dark-evening-176851.jpg" width="80%" height="80%"></p><p align="right">这也太黑了吧</p><h2 id="Sublime-Text"><a href="#Sublime-Text" class="headerlink" title="Sublime Text"></a>Sublime Text</h2><p><a href="https://dribbble.com/shots/3914831-Sublime-Text-Icon-Sketch-and-icns-Files" target="_blank" rel="noopener">换了个图标</a><br><img src="sublime.png" width="20%" height="20%"><br>和<a href="https://skim-app.sourceforge.io/" target="_blank" rel="noopener">Skim</a>结合，编译可视化LaTeX</p><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><ol><li>Caffe Prototxt Syntax</li><li>CUDA C++</li><li>Julia</li><li>Latex-cwl</li><li>LatexTools</li><li>Package Control</li><li>Protobuf Syntal Hightlighting</li><li>Protocol Buffer Syntax</li><li>SFTP</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> Software </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Transposed Convolution in PyTorch</title>
      <link href="/2019/05/21/Transposed-Convolution-in-PyTorch/"/>
      <url>/2019/05/21/Transposed-Convolution-in-PyTorch/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>Convolution运算的一些参数我们结合图示比较容易理解，卷积核在feature map中滑动，对应元素乘加得到输出。但是Transposed Convolution具体是怎么运算的，其中的参数配置又有什么具体的作用，需要我们初学者深入研究一下。</p><p><img src="no_padding_strides.GIF" width="20%" height="20%"><br>如图所示，对于Convolution来说，下层为输入，上层为输出；对于Transposed Convolution来说，为Convolution的逆运算或者求导运算，上层为输入，下层为输出。</p><h1 id="Transposed-Convolution-in-PyTorch"><a href="#Transposed-Convolution-in-PyTorch" class="headerlink" title="Transposed Convolution in PyTorch"></a>Transposed Convolution in PyTorch</h1><h2 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode=&apos;zeros&apos;)</span><br></pre></td></tr></table></figure><p>与公式中变量的对应关系为：</p><table><thead><tr><th>Param</th><th>Variable name</th></tr></thead><tbody><tr><td>in_channels</td><td>$c_{in}$</td></tr><tr><td>out_channels</td><td>$c_{out}$</td></tr><tr><td>kernel_size</td><td>$k$</td></tr><tr><td>stride</td><td>$s$</td></tr><tr><td>padding</td><td>$p$</td></tr><tr><td>output_padding</td><td>$op$</td></tr><tr><td>groups</td><td>$g$</td></tr><tr><td>dilation</td><td>$d$</td></tr></tbody></table><p>$$<br>L_{out} = (L_{in} - 1) * s - 2 \times p + d \times (k - 1) + op + 1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: deconv</span><br><span class="line">Out[<span class="number">14</span>]: ConvTranspose1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>,), stride=(<span class="number">1</span>,), bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: x</span><br><span class="line">Out[<span class="number">15</span>]: tensor([[[<span class="number">1.</span>, <span class="number">1.</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: deconv.weight</span><br><span class="line">Out[<span class="number">16</span>]: </span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]], requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: y = deconv(x)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: y</span><br><span class="line">Out[<span class="number">18</span>]: tensor([[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">1.</span>]]], grad_fn=&lt;SqueezeBackward1&gt;)</span><br></pre></td></tr></table></figure><p><img src="IMG_0241.jpg" width="20%" height="20%"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">22</span>]: x</span><br><span class="line">Out[<span class="number">22</span>]: tensor([[[<span class="number">1.</span>, <span class="number">1.</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: deconv</span><br><span class="line">Out[<span class="number">23</span>]: ConvTranspose1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>,), stride=(<span class="number">2</span>,), bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: deconv.weight</span><br><span class="line">Out[<span class="number">24</span>]: </span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]], requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: y = deconv(x)</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: y</span><br><span class="line">Out[<span class="number">26</span>]: tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]], grad_fn=&lt;SqueezeBackward1&gt;)</span><br></pre></td></tr></table></figure><p><img src="IMG_0242.jpg" width="20%" height="20%"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">30</span>]: x</span><br><span class="line">Out[<span class="number">30</span>]: tensor([[[<span class="number">1.</span>, <span class="number">1.</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">31</span>]: deconv</span><br><span class="line">Out[<span class="number">31</span>]: ConvTranspose1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>,), stride=(<span class="number">4</span>,), bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: deconv.weight.data</span><br><span class="line">Out[<span class="number">32</span>]: tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: y = deconv(x)</span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: y</span><br><span class="line">Out[<span class="number">34</span>]: tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]], grad_fn=&lt;SqueezeBackward1&gt;)</span><br></pre></td></tr></table></figure><p><img src="IMG_0243.jpg" width="20%" height="20%"></p><h2 id="Convert-Transposed-Convolution-to-Convolution"><a href="#Convert-Transposed-Convolution-to-Convolution" class="headerlink" title="Convert Transposed Convolution to Convolution"></a>Convert Transposed Convolution to Convolution</h2><div class="row">    <embed src="deconv.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
      
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> Transposed Convolution </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Ubuntu environment configuration</title>
      <link href="/2019/05/02/ubuntu-environment-configuration/"/>
      <url>/2019/05/02/ubuntu-environment-configuration/</url>
      
        <content type="html"><![CDATA[<h2 id="Update-etc-apt-source-list"><a href="#Update-etc-apt-source-list" class="headerlink" title="Update /etc/apt/source.list"></a>Update /etc/apt/source.list</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/apt/source.list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释</span></span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预发布软件源，不建议启用</span></span><br><span class="line"><span class="comment"># deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span></span><br><span class="line"><span class="comment"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span></span><br><span class="line"><span class="comment"># deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable</span></span><br><span class="line"><span class="comment"># deb-src [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable</span></span><br><span class="line"><span class="comment"># deb-src [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable</span></span><br><span class="line"><span class="comment"># deb-src [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable</span></span><br></pre></td></tr></table></figure><h2 id="Base-software"><a href="#Base-software" class="headerlink" title="Base software"></a>Base software</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vim</span><br><span class="line">sudo apt-get install screen</span><br><span class="line">sudo apt-get install git</span><br><span class="line">sudo apt-get install zsh</span><br><span class="line">sh -c <span class="string">"<span class="variable">$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)</span>"</span></span><br><span class="line">sudo apt-get install htop</span><br><span class="line">sudo apt-get install graphviz</span><br><span class="line">sudo apt-get install unrar</span><br></pre></td></tr></table></figure><h2 id="CUDA9-0"><a href="#CUDA9-0" class="headerlink" title="CUDA9.0"></a>CUDA9.0</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/ict/Downloads/cuda9</span><br><span class="line"><span class="comment"># nvidia-driver</span></span><br><span class="line">sudo dpkg -i nvidia-driver-local-repo-ubuntu1604-387.34_1.0-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda-drivers</span><br><span class="line">sudo reboot</span><br><span class="line"><span class="comment"># cuda9.0</span></span><br><span class="line"><span class="built_in">cd</span> /home/ict/Downloads/cuda9</span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda</span><br><span class="line"><span class="comment"># cudnn</span></span><br><span class="line">sudo dpkg -i libcudnn7-dev_7.1.3.16-1+cuda9.0_amd64.deb</span><br><span class="line">sudo dpkg -i libcudnn7_7.1.3.16-1+cuda9.0_amd64.deb</span><br></pre></td></tr></table></figure><h2 id="Remove-CUDA"><a href="#Remove-CUDA" class="headerlink" title="Remove CUDA"></a>Remove CUDA</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove <span class="string">"cuda-*"</span></span><br><span class="line">sudo apt-get --purge remove <span class="string">"nvidia-*"</span></span><br></pre></td></tr></table></figure><h2 id="Using-the-specified-GPU"><a href="#Using-the-specified-GPU" class="headerlink" title="Using the specified GPU"></a>Using the specified GPU</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0,1"</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> cuda </tag>
            
            <tag> nvidia-driver </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Paper note: Tensorflow Lite: Speeding up mobile inference with low precision</title>
      <link href="/2019/04/12/paper-note-Tensorflow-Lite-Speeding-up-mobile-inference-with-low-precision/"/>
      <url>/2019/04/12/paper-note-Tensorflow-Lite-Speeding-up-mobile-inference-with-low-precision/</url>
      
        <content type="html"><![CDATA[<h1 id="Tensorflow-Lite-Speeding-up-mobile-inference-with-low-precision"><a href="#Tensorflow-Lite-Speeding-up-mobile-inference-with-low-precision" class="headerlink" title="Tensorflow Lite: Speeding up mobile inference with low precision"></a>Tensorflow Lite: Speeding up mobile inference with low precision</h1><h2 id="Quantization-scheme"><a href="#Quantization-scheme" class="headerlink" title="Quantization scheme"></a>Quantization scheme</h2><p>$$r = S(q-Z)$$ where constants $S$ and $Z$ are quantization parameters, intergers $q$ are mapped to real numbers $r$.</p><table><thead><tr><th>param</th><th></th><th>type</th></tr></thead><tbody><tr><td>S</td><td>Scale</td><td>fp32</td></tr><tr><td>q</td><td>quantize</td><td>uint8(w and a) int32(b)</td></tr><tr><td>Z</td><td>Zero-point</td><td>uint8(w and a) 0(b)</td></tr></tbody></table><p>Note: </p><ul><li>怎么表示负数?<br>通过设置合适的$Z$来表示负数</li></ul><h2 id="Integer-arithmetic-only-matrix-multiplication"><a href="#Integer-arithmetic-only-matrix-multiplication" class="headerlink" title="Integer-arithmetic-only matrix multiplication"></a>Integer-arithmetic-only matrix multiplication</h2><p><span class="image-caption">我们考虑weight矩阵的第i行，与activation矩阵的第k列相乘，如下图所示</span><br><img src="mul_figure.jpg" width="50%" height="50%"></p><p><span class="image-caption">直接计算的话，需要将uint8扩展为int16，进行乘法运算</span><br><img src="google8bit_mul.png" width="50%" height="50%"></p><p><span class="image-caption">将原来的运算展开后，便可以使用uint8的乘法进行运算</span><br><img src="google8bit_mul_up.png" width="50%" height="50%"></p><h2 id="Learning-quantization-parameter"><a href="#Learning-quantization-parameter" class="headerlink" title="Learning quantization parameter"></a>Learning quantization parameter</h2><p>以量化到uint8举例，$2^8=256$一共能表示256个大小不同的数，表示的范围为$[a, b]$，先定义如下函数：<br><img src="google8bit_clamp.png" width="50%" height="50%"><br>由于我们采用uint8，所以$n=2^8=256$，所以只需要确定$[a, b]$就可以确定量化的参数$S, Z$<br>$$<br>    S = \frac{b-a}{2^8} \\<br>    Z = \frac{2^8a}{a-b}<br>$$</p><ul><li>weight<br>对于w来说，只是简单求min和max得到$[a, b]$</li><li>activation<br>对于a来说，在traning时使用exponential moving average(EMA)的方式获取$[a, b]$</li></ul><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><p><img src="google8bit_workflow.png" width="50%" height="50%"><br>Note:<br>这个量化流程中，需要进行重新训练，不过按照上边的道理分析，对于8bits不进行训练最后的结果应该也不会太差，对于更低bit的量化，重新训练则是十分重要的。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>主要是accuracy 和latency之间的tradeoff，在ARM cpu上进行latency测试<br><img src="google8bit_resnet.png" width="50%" height="50%"><br><img src="google8bit_inception.png" width="50%" height="50%"><br>说句实在话，8bits量化的效果不怎么好呀，而且是uint8，再加上一个$Z$,相当于9bit了，但是结果accuracy下降了好几个点。下面是Qcode方式量化的结果，两个网络结构不同，还没来得及做更多的实验，但可以对比一下:<br><img src="qcode8bit.png" width="80%" height="80%"></p><h2 id="Comparison-with-Qcode-method"><a href="#Comparison-with-Qcode-method" class="headerlink" title="Comparison with Qcode method"></a>Comparison with Qcode method</h2><table><thead><tr><th>Features</th><th>Google8bits</th><th>Qcode 8bits</th></tr></thead><tbody><tr><td>scheme</td><td>$r = S(q-z)$ type(q)=uint8</td><td>$r = 2^nq$ type(q)=int8</td></tr><tr><td>quantized training</td><td>True</td><td>False</td></tr><tr><td>BN fusion</td><td>True</td><td>True</td></tr><tr><td>ARM cpu latency experiment</td><td>True</td><td>False</td></tr><tr><td>More experiment(face detection)</td><td>True</td><td>TODO</td></tr><tr><td>Hardware friendly</td><td>False</td><td>True</td></tr></tbody></table><h1 id="Inferences"><a href="#Inferences" class="headerlink" title="Inferences"></a>Inferences</h1><ol><li><a href="https://arxiv.org/abs/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Quantization </tag>
            
            <tag> paper note </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>BN_Fusion</title>
      <link href="/2019/02/26/BN-Fusion/"/>
      <url>/2019/02/26/BN-Fusion/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在PyTorch中，我们常在网络中遇到BN层，基本单元如果如下所示，则可以离线将Conv和BN进行fusion，从而在inference时不必计算BN。</p><p><img src="conv_bn.svg" alt="conv_bn"></p><p>在PyTorch中<code>BatchNorm2d</code>的定义如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">BatchNorm2d</span><span class="params">(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span></span></span><br></pre></td></tr></table></figure></p><p>参数列表如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([<span class="string">'weight'</span>, <span class="string">'bias'</span>, <span class="string">'running_mean'</span>, <span class="string">'running_var'</span>, <span class="string">'num_batches_tracked'</span>])</span><br></pre></td></tr></table></figure></p><p>与公式中变量的对应关系为：</p><table><thead><tr><th>Param</th><th>Variable name</th></tr></thead><tbody><tr><td>eps</td><td>$\epsilon$</td></tr><tr><td>weight</td><td>$\gamma$</td></tr><tr><td>bias</td><td>$\beta$</td></tr><tr><td>running_mean</td><td>$\mu$</td></tr><tr><td>running_var</td><td>$\sigma^2$</td></tr></tbody></table><h1 id="Key-Idea"><a href="#Key-Idea" class="headerlink" title="Key Idea"></a>Key Idea</h1><p>Convolution layer:<br>$$<br>    Y = X * w + b<br>$$<br>BatchNorm layer:<br>$$<br>    Y = \frac{X - \mu}{\sqrt{\sigma^2 + \epsilon}}  \gamma + \beta<br>$$ </p><p>Convolution and BatchNorm fusion:</p><p>$$<br>    Y = \frac{(X * w + b) - \mu}{\sqrt{\sigma^2 + \epsilon}}  \gamma + \beta \\<br>    Y = \frac{\gamma w}{\sqrt{\sigma^2 + \epsilon}}X + \frac{b - \mu}{\sqrt{\sigma^2 + \epsilon}}\gamma + \beta \\<br>    w_{merged} = aw; \quad<br>    b_{merged} = (b - \mu)a + \beta; \quad<br>    a = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}}<br>$$</p><h2 id="Uniform-Quantization"><a href="#Uniform-Quantization" class="headerlink" title="Uniform Quantization"></a>Uniform Quantization</h2><p>$$r = Sq$$ where constants $S$ is quantization parameter, intergers $q$ are mapped to real numbers $r$.</p><table><thead><tr><th>param</th><th></th><th>type</th></tr></thead><tbody><tr><td>S</td><td>Scale</td><td>fp32</td></tr><tr><td>q</td><td>quantize</td><td>int4 for w int5 for a</td></tr></tbody></table><p>Quantized convolution layer:<br>$$<br>    Y = X * Sw_q + b \\<br>    where: w = Sw_q<br>$$</p><p>Quantized convolution and BatchNorm fusion:</p><p>$$<br>    Y = \frac{(X * Sw_q + b) - \mu}{\sqrt{\sigma^2 + \epsilon}}  \gamma + \beta \\<br>    Y = \frac{\gamma Sw_q}{\sqrt{\sigma^2 + \epsilon}}X + \frac{b - \mu}{\sqrt{\sigma^2 + \epsilon}}\gamma + \beta \\<br>    S_{merged} = aS; \quad<br>    w_{merged} = aw; \quad<br>    b_{merged} = (b - \mu)a + \beta; \quad<br>    a = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}}<br>$$</p><h1 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h1><ol><li><a href="https://pytorch.org/docs/stable/nn.html?highlight=batchnorm#torch.nn.BatchNorm2d" target="_blank" rel="noopener">Docs: torch.nn.BatchNorm2d</a></li><li><a href="https://www.jianshu.com/p/e042d693f3fb" target="_blank" rel="noopener">BN层合并原理及实现</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> Batch Normalization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>suvery-of-qcnn</title>
      <link href="/2019/01/16/suvery-of-qcnn/"/>
      <url>/2019/01/16/suvery-of-qcnn/</url>
      
        <content type="html"><![CDATA[<h1 id="Quantization-Algorithms"><a href="#Quantization-Algorithms" class="headerlink" title="Quantization Algorithms"></a>Quantization Algorithms</h1><h2 id="Truncation-without-re-training"><a href="#Truncation-without-re-training" class="headerlink" title="Truncation without re-training"></a>Truncation without re-training</h2><h3 id="原理与分析"><a href="#原理与分析" class="headerlink" title="原理与分析"></a>原理与分析</h3><p>将fp32的数，直接截断为8bit定点数<br>以8bits为例，每层的权重共享一个QCode，每层的activation共享一个QCode。<br>$$<br>    x_q = \frac{clip(round(x_f\times 2^{qcode}), -2^7, 2^7-1)}{2^{qcode}}<br>$$<br><img src="qcode.svg" alt="qcode1"></p><h4 id="如何配置QCode"><a href="#如何配置QCode" class="headerlink" title="如何配置QCode"></a>如何配置QCode</h4><ol><li><p>accuracy-aware config<br>对于分类网络来说，我们只关心它最后的分类精度，比如baseline网络在ImageNet上分类精度为70%，通过改变配置config，能够得到最后量化后的网络的分类精度。配置Qcode的指标只关心最后的分类精度。</p></li><li><p>Greedy layer-wise optimization<br>另外，可以采用更加方便的方式，通过最小化$x_q$与$x$之间的KL散度或者MSE，来配置每层的QCode.</p></li></ol><h3 id="特点分析"><a href="#特点分析" class="headerlink" title="特点分析"></a>特点分析</h3><ol><li>运算简单，不需要重新进行训练</li><li>需要逐层配置QCode参数，耗费较多资源</li><li>QCode的量化编码方式过于简单，且Qcode只能取值为整数，对分类网络比较友好，但是对检测网络和其他类型的网络并不一定能轻松适配。</li></ol><h3 id="试验与结论"><a href="#试验与结论" class="headerlink" title="试验与结论"></a>试验与结论</h3><p>使用这种方式进行离线量化，最后的分类网络的精度基本不会下降（千分位的变动）.</p><h2 id="DoReFa-Net"><a href="#DoReFa-Net" class="headerlink" title="DoReFa Net"></a>DoReFa Net</h2><p>For weights:<br>$$<br>    w^k = 2 quantize_k(\frac{\tanh(w_i)}{2 \max(|\tanh(w_i)|)} + \frac{1}{2}) - 1<br>$$<br>uniform quantization :( 为什么不直接使用clip函数，明明最后用的是[-1, 1]之间的数值，$tanh$的作用感觉也没有那么大；在某些已经训练好的网络中，weight的分布可能是非常集中在0附近的，如果仍然使用这个映射方式，就导致该层的weight分布强行被拉到[-1, 1]之间，那该层的功能性是不是就改变了？这是不是 DoReFa类型的量化对 first and last layer 敏感的原因呢？<br>For activations:<br>$$<br>    a^k = quantize_k(clip(r, 0, 1))<br>$$<br>uniform quantization :(</p><h2 id="Towards-Effective-Low-bitwidth-Convolutional-Neural-Networks"><a href="#Towards-Effective-Low-bitwidth-Convolutional-Neural-Networks" class="headerlink" title="Towards Effective Low-bitwidth Convolutional Neural Networks"></a>Towards Effective Low-bitwidth Convolutional Neural Networks</h2><p>The quantization equation:<br>$$<br>    z_q = Q(z_r) = \frac{1}{2^k - 1}round((2^k - 1)z_r)<br>$$<br>where $z_r \in [0, 1]$ denotes the full-precision value and $z_q \in [0, 1]$denotes the quantized value.</p><p>For weights:<br>$$<br>    w_q = Q(\frac{\tanh(w)}{2\max(|\tanh(w)|)} + \frac{1}{2})<br>$$<br>应该是paper中写错了，最后应该和DoFeFa的量化方式一样<br>For activation:<br>$$<br>    x_q = Q(clip(x, 0, 1))<br>$$<br>Paper 主要做了一下几组实验：<br><strong>TS</strong> Two step: quantize weight =&gt; quantize activation<br><strong>PQ</strong> Progressive quantization: quantization from higher precisions to lower precisions<br><strong>Guided</strong> Teacher student training.</p><h2 id="WRPN"><a href="#WRPN" class="headerlink" title="WRPN"></a>WRPN</h2><p>受到了Wide ResNet的启发，Wide ResNet减少网络的深度，扩展网络的宽度，通过重新设计了网络结构的方式来使得网络保持原有的精度。<strong>WRPN</strong> wide reduced-precision networks需要对weights和activation进行量化，相当于减少了网络的拟合能力，可以通过增加网络宽度的方式来增加一部分参数来弥补量化带来的损失。</p><p>半成品的实现：<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Code</a></p><h3 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h3><ol><li>对w进行[-1, 1]截断，对a进行[0, 1]截断，截断使用<a href="https://www.tensorflow.org/api_docs/python/tf/clip_by_value" target="_blank" rel="noopener">clip_by_value</a>函数；</li><li>对w进行有符号量化，对a进行无符号量化<br>$$<br> w_q = \frac{1}{2^{k-1}-1}round((2^{k-1}-1)\cdot w_f) \\<br> a_q = \frac{1}{2^{k}-1}round((2^{k}-1)\cdot a_f)<br>$$</li></ol><h3 id="特点分析-1"><a href="#特点分析-1" class="headerlink" title="特点分析"></a>特点分析</h3><ol><li>quantier的parameter是静态的，不需要像TTQ一样需要在训练中进行学习</li><li><code>In our work, we maintain the depth parameter same as baseline network but widen the filter maps.</code></li><li><code>To be consistent with results reported in prior works, we do not quantize weights and activations of the first and last layer.</code></li><li>最后是在Intel Arria 10 进行了FPGA的实现，<a href="https://arxiv.org/pdf/1806.11547.pdf" target="_blank" rel="noopener">Exploration of Low Numeric Precision Deep Learning Inference Using Intel ® FPGAs</a>论文中又有了更加详细的实现细节。</li></ol><h3 id="试验与结论-1"><a href="#试验与结论-1" class="headerlink" title="试验与结论"></a>试验与结论</h3><p><img src="WRPN_result.png" alt="resnet"></p><h2 id="VNQ-Variational-Network-Quantization"><a href="#VNQ-Variational-Network-Quantization" class="headerlink" title="VNQ: Variational Network Quantization"></a>VNQ: Variational Network Quantization</h2><p><code>Our method is an extension of Sparse VD</code> 公式很多，还需要时间推导。</p><h3 id="原理分析-1"><a href="#原理分析-1" class="headerlink" title="原理分析"></a>原理分析</h3><h3 id="特点分析-2"><a href="#特点分析-2" class="headerlink" title="特点分析"></a>特点分析</h3><ol><li><code>The method does not require fine-tuning after quantization.</code><br>可以从scratch开始训练，也可以直接使用预训练好的权重。</li><li><code>Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).</code><br>只有小数据集小网络，小数据集大网络，没有大数据集和大网络，可能不具有普适性。</li></ol><h2 id="WAGE-Training-and-Inference-with-Integers-in-Deep-Neural-Networks"><a href="#WAGE-Training-and-Inference-with-Integers-in-Deep-Neural-Networks" class="headerlink" title="WAGE: Training and Inference with Integers in Deep Neural Networks"></a>WAGE: Training and Inference with Integers in Deep Neural Networks</h2><p>官方源码：<a href="https://github.com/boluoweifenda/WAGE" target="_blank" rel="noopener">code</a>,只包含cifar10的demo。<br>将Weight,Activation,Gradient,Error都进行量化，量化方式采用<br>$$<br>    \sigma(k) = 2^{k-1}, k\in \mathbb{N_+} \\<br>    Q(x, k) = clip(\sigma(k)\cdot round(\frac{x}{\sigma(k)}), -1+\sigma(k), 1-\sigma(k))<br>$$</p><h3 id="试验与结论-2"><a href="#试验与结论-2" class="headerlink" title="试验与结论"></a>试验与结论</h3><p><img src="WAGE_result.png" alt="result"><br>如果需要在AI芯片进行训练，WAGE是一个很好的研究方向；但是当前主流的做法是训练交给GPU，只需要在AI芯片上进行inference.</p><h2 id="Clip-Q-Deep-network-compression-learning-by-In-Parallel-Pruning-Quantization"><a href="#Clip-Q-Deep-network-compression-learning-by-In-Parallel-Pruning-Quantization" class="headerlink" title="Clip-Q: Deep network compression learning by In-Parallel Pruning Quantization"></a>Clip-Q: Deep network compression learning by In-Parallel Pruning Quantization</h2><ol><li><code>We combine network pruning and weight quantization in a single learning framework</code></li><li>使用贝叶斯优化器来<code>the pruning rate p and the bit budget b</code></li><li>只对weights进行了pruning和quantization. pruning是百分比的方式，quantization是聚类codebook的方式</li></ol><h3 id="试验与结论-3"><a href="#试验与结论-3" class="headerlink" title="试验与结论"></a>试验与结论</h3><p><img src="clipq_result.png" alt="clipq_result.png"><br>量化相关试验我认为还是比较相同bitwidth下的精度更切合实际应用，毕竟量化的作用一方面是压缩权重，更重要的还是AI芯片的inference加速，需要给出hardware friendly的分析与验证。</p><h2 id="Bi-Real-Net"><a href="#Bi-Real-Net" class="headerlink" title="Bi-Real Net"></a>Bi-Real Net</h2><ol><li>较XOR-Net, BNN网络的分类accuracy有较大的提升</li><li>没有量化第一层和最后一层，并不<code>Real</code>:)</li></ol><h3 id="试验与结论-4"><a href="#试验与结论-4" class="headerlink" title="试验与结论"></a>试验与结论</h3><p><img src="bi_real_result.png" alt="bi_real_result.png"></p><h2 id="Efficient-Non-uniform-quantizer-for-quantized-neural-network-targeting-Re-configurable-hardware"><a href="#Efficient-Non-uniform-quantizer-for-quantized-neural-network-targeting-Re-configurable-hardware" class="headerlink" title="Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware"></a>Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware</h2><p><img src="non_uniform.png" alt="non_uniform.png"></p><h2 id="ELQ-Explicit-Loss-Error-Aware-Quantization-for-Low-Bit-Deep-Neural-Networks"><a href="#ELQ-Explicit-Loss-Error-Aware-Quantization-for-Low-Bit-Deep-Neural-Networks" class="headerlink" title="ELQ: Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks"></a>ELQ: Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks</h2><ol><li>之前的方式大多是layer-wise 最小化量化前后数值的    Error，本文建立了quantization与loss之间的关系。<br>其实在pruning中也有类似的改进，一般大家的认为weights越小越不重要，优先prune掉，后来提出fisher pruning就是根据loss来确定pruning的策略。前者简单直接，后者理论上能找到更优的解，但实现麻烦。</li><li>只量化了weights</li></ol><h2 id="PACT-parameterized-clipping-activation-for-quantized-neural-networks"><a href="#PACT-parameterized-clipping-activation-for-quantized-neural-networks" class="headerlink" title="PACT: parameterized clipping activation for quantized neural networks"></a>PACT: parameterized clipping activation for quantized neural networks</h2><ol><li>PACT提出Activation的量化方法，并使用DoReFa量化weight，最终训练w4a4    网络在ImageNet上仍能达到很好的accuracy，编码方式也十分简单，很值得借鉴。</li><li>仍然没有量化第一层和最后一层</li></ol><h3 id="原理分析-2"><a href="#原理分析-2" class="headerlink" title="原理分析"></a>原理分析</h3><ol><li>Forward quantization<br>$$<br> y = PACT(x) = 0.5(|x|-|x-\alpha|+\alpha) \\<br> y = round(y\cdot \frac{2^k-1}{\alpha})\cdot \frac{\alpha}{2^k-1}<br>$$<br>这里也看出k对应的是unsigned int的位数。<br>在ResNet的bottle net中，含有没有ReLU的feature map, 在一些比较常用的物体检测网络比如YOLO中，采用了leacky ReLU, 在PACT中，并没有对其进行特殊的考虑(它只考虑了无符号的情况)。</li><li>Backward, STE:<br>$$<br> \cfrac{\partial y_q}{\partial\alpha} = \cfrac{\partial y_q}{\partial y}\cfrac{\partial y}{\partial\alpha}\simeq \cfrac{\partial y}{\partial \alpha}<br>$$</li></ol><h3 id="试验与结论-5"><a href="#试验与结论-5" class="headerlink" title="试验与结论"></a>试验与结论</h3><p><img src="pact_result.png" alt="pact_result.png"><br>weights使用DoReFa方式量化，activations使用PACT方式量化，可以看到在ResNet18上w4a4只和baseline低了一个点。</p><h2 id="HWGQ-Deep-Learning-with-Low-Precision-by-Half-wave-Gaussian-Quantization-CVPR2017"><a href="#HWGQ-Deep-Learning-with-Low-Precision-by-Half-wave-Gaussian-Quantization-CVPR2017" class="headerlink" title="HWGQ: Deep Learning with Low Precision by Half-wave Gaussian Quantization. CVPR2017"></a>HWGQ: Deep Learning with Low Precision by Half-wave Gaussian Quantization. CVPR2017</h2><p>主要是w1a2</p><h2 id="FQN-Fully-Quantized-Network-for-Object-Detection-CVPR2019"><a href="#FQN-Fully-Quantized-Network-for-Object-Detection-CVPR2019" class="headerlink" title="FQN: Fully Quantized Network for Object Detection. CVPR2019"></a>FQN: Fully Quantized Network for Object Detection. CVPR2019</h2><p>主要在物体检测网络上进行了w4a4的量化实验，效果比较好</p><p>$$<br>    X^Q = Q_k(X^R)\in\{q_0, q_1, q_2, \dots, q_{2^k-1}\} \\<br>    X^Q = \Delta(X^I - z) \\<br>    lb = q_0, ub = q_{2^k-1} \\<br>    \Delta = \frac{ub - lb}{2^k-1}<br>$$</p><p>For weight:<br>$$<br>    lb = \min(W) \\<br>    ub = \max(W)<br>$$</p><p>For activation:<br>$$<br>    lb^l_{EMA} = EMA(\min(A), \gamma=0.999) \\<br>    ub^l_{EMA} = EMA(\max(A), \gamma=0.999)<br>$$<br>为了减少activation的波动性，作者将bn freeze住，并在后续的实验部分验证了做法的有效性。<br>具体做法为，将bn的参数修正到与之对应的conv和fc的weight和bias中，量化时是对修正后的$weight_{merged}$进行量化，训练时更新weight，而不是更新$weight_{merged}$</p><p>如何integer only计算卷积<br>$$<br>    y = Q_k(A^R)\times Q_k(W^R) \\<br>      = \Delta_a(A^I - z_a) \times \Delta_w(W^I - z_w) \\<br>      = \Delta_a  \Delta_w (A^I W^I - A^I z_w - W^I z_a + z_a z_w)<br>$$</p><h1 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h1><ol><li><a href="https://joyeeo.github.io/2019/01/11/paper-of-quantization/">paper-of-quantization</a></li><li><a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">nervanasystems.github.io</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> quantization </tag>
            
            <tag> CNN </tag>
            
            <tag> compression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mxnet-build-from-source</title>
      <link href="/2019/01/08/mxnet-build-from-source/"/>
      <url>/2019/01/08/mxnet-build-from-source/</url>
      
        <content type="html"><![CDATA[<h1 id="Build-MXNet-from-Source"><a href="#Build-MXNet-from-Source" class="headerlink" title="Build MXNet from Source"></a>Build MXNet from Source</h1><h2 id="Clone-the-MXNet-Project"><a href="#Clone-the-MXNet-Project" class="headerlink" title="Clone the MXNet Project"></a>Clone the MXNet Project</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recursive https://github.com/apache/incubator-mxnet mxnet</span><br><span class="line"><span class="built_in">cd</span> mxnet</span><br></pre></td></tr></table></figure><h2 id="Download-mklcudnn"><a href="#Download-mklcudnn" class="headerlink" title="Download mklcudnn"></a>Download mklcudnn</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> 3rdparty/mkldnn/external</span><br><span class="line">wget https://github.com/intel/mkl-dnn/releases/download/v0.17.2/mklml_lnx_2019.0.1.20180928.tgz</span><br></pre></td></tr></table></figure><h2 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> docs/install </span><br><span class="line">./install_mxnet_ubuntu_python.sh</span><br></pre></td></tr></table></figure><h2 id="install-python"><a href="#install-python" class="headerlink" title="install python"></a>install python</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> python</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h2 id="Add-operator-in-backend"><a href="#Add-operator-in-backend" class="headerlink" title="Add operator in backend"></a>Add operator in backend</h2><h3 id="Why-not-add-custom-operator-using-PythonOp-interface"><a href="#Why-not-add-custom-operator-using-PythonOp-interface" class="headerlink" title="Why not add custom operator using PythonOp interface."></a>Why not add custom operator using <code>PythonOp</code> interface.</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NDArrayOp</span><span class="params">(PythonOp)</span>:</span></span><br><span class="line">    <span class="string">"""Base class for numpy operators. numpy operators allow parts</span></span><br><span class="line"><span class="string">    of computation in symbolic graph to be writen in numpy. This feature</span></span><br><span class="line"><span class="string">    is intended for quickly hacking out a solution for non performance</span></span><br><span class="line"><span class="string">    critical parts. Please consider write a c++ implementation if it becomes</span></span><br><span class="line"><span class="string">    a bottleneck.</span></span><br><span class="line"><span class="string">    Note that if your operator contains internal states (like arrays),</span></span><br><span class="line"><span class="string">    it cannot be used for multi-gpu training.</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure><p><a href="https://cwiki.apache.org/confluence/display/MXNET/CLion+setup+for+MXNet+development+on+Mac" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/MXNET/CLion+setup+for+MXNet+development+on+Mac</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://mxnet.incubator.apache.org/install/ubuntu_setup.html#install-mxnet-for-python" target="_blank" rel="noopener">install-mxnet-for-python</a><br><a href="https://mxnet.incubator.apache.org/faq/add_op_in_backend.html" target="_blank" rel="noopener">add op in backend</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> MXNet </tag>
            
            <tag> GLUON </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>install-tensorRT</title>
      <link href="/2018/10/26/install-tensorRT/"/>
      <url>/2018/10/26/install-tensorRT/</url>
      
        <content type="html"><![CDATA[<h1 id="Install-Driver-and-CUDA"><a href="#Install-Driver-and-CUDA" class="headerlink" title="Install Driver and CUDA"></a>Install Driver and CUDA</h1><p><a href="https://joyeeo.github.io/2018/10/18/install-cuda/">install cuda</a></p><h1 id="Install-TensorRT"><a href="#Install-TensorRT" class="headerlink" title="Install TensorRT"></a>Install TensorRT</h1><h2 id="Download-TensorRT-5-0"><a href="#Download-TensorRT-5-0" class="headerlink" title="Download TensorRT 5.0"></a>Download TensorRT 5.0</h2><p><a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener">tensorrt download</a></p><p>Then, I got the following package:<br><code>nv-tensorrt-repo-ubuntu1604-cuda9.0-trt5.0.0.10-rc-20180906_1-1_amd64.deb</code></p><h2 id="Install-TensorRT-5-0"><a href="#Install-TensorRT-5-0" class="headerlink" title="Install TensorRT 5.0"></a>Install TensorRT 5.0</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i nv-tensorrt-repo-ubuntu1604-cuda9.0-trt5.0.0.10-rc-20180906_1-1_amd64.deb</span><br><span class="line">sudo apt-key add /var/nv-tensorrt-repo-cuda9.0-trt5.0.0.10-rc-20180906/7fa2af80.pub  </span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment"># Unluckily, I encountered the following problem.</span></span><br><span class="line"><span class="comment">## double free or corruption (fasttop): 0x0000000001368e00 ***</span></span><br><span class="line"><span class="comment"># I solved it by run: sudo apt-get purge libappstream3 </span></span><br><span class="line">sudo apt-get install tensorrt</span><br></pre></td></tr></table></figure><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/src/tensorrt/samples</span><br><span class="line">sudo make -j32</span><br><span class="line"><span class="built_in">cd</span> ../bin</span><br><span class="line">./samples_mnist</span><br></pre></td></tr></table></figure><p><img src="mnist.png" alt="mnist"></p><h2 id="Install-PyCUDA"><a href="#Install-PyCUDA" class="headerlink" title="Install PyCUDA"></a>Install PyCUDA</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pip install pycuda</span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># In file included from src/cpp/cuda.cpp:1:0:</span></span><br><span class="line"><span class="comment">#     src/cpp/cuda.hpp:14:18: fatal error: cuda.h: No such file or directory</span></span><br><span class="line"><span class="comment">#     compilation terminated.</span></span><br><span class="line"><span class="comment">#     error: command 'gcc' failed with exit status 1</span></span><br><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda/bin:<span class="variable">$PATH</span></span><br><span class="line">pip install pycuda</span><br></pre></td></tr></table></figure><h3 id="uff-custom-plugin"><a href="#uff-custom-plugin" class="headerlink" title="uff custom plugin"></a>uff custom plugin</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/src/tensorrt/samples/python/uff_custom_plugin</span><br><span class="line">mkdir build &amp;&amp; pushd build</span><br><span class="line">cmake ..</span><br><span class="line">make -j8</span><br><span class="line">python2 lenet5.py</span><br><span class="line">python2 mnist_uff_custom_plugin.py</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> cuda </tag>
            
            <tag> nvidia-driver </tag>
            
            <tag> ubuntu </tag>
            
            <tag> tensorRT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>git-command</title>
      <link href="/2018/09/29/git-command/"/>
      <url>/2018/09/29/git-command/</url>
      
        <content type="html"><![CDATA[<h1 id="Git-command"><a href="#Git-command" class="headerlink" title="Git command"></a>Git command</h1><h2 id="update-a-forked-repo-from-remote-repo"><a href="#update-a-forked-repo-from-remote-repo" class="headerlink" title="update a forked repo from remote repo."></a>update a forked repo from remote repo.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">git remote add upstream git@github.com:&lt;custom&gt;.git</span><br><span class="line"></span><br><span class="line">git remote -v</span><br><span class="line"></span><br><span class="line">git fetch upstream</span><br><span class="line"></span><br><span class="line">git merge upstream/master</span><br><span class="line"></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><h2 id="git-合并多个commits"><a href="#git-合并多个commits" class="headerlink" title="git 合并多个commits"></a>git 合并多个commits</h2><p><a href="https://www.jianshu.com/p/964de879904a" target="_blank" rel="noopener">合并多个 Commit</a></p><h2 id="git拉取远程分支到本地"><a href="#git拉取远程分支到本地" class="headerlink" title="git拉取远程分支到本地"></a>git拉取远程分支到本地</h2><h3 id="查看远程分支"><a href="#查看远程分支" class="headerlink" title="查看远程分支"></a>查看远程分支</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -r</span><br></pre></td></tr></table></figure><h3 id="拉取远程分支到本地分支"><a href="#拉取远程分支到本地分支" class="headerlink" title="拉取远程分支到本地分支"></a>拉取远程分支到本地分支</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b 本地分支名x origin/远程分支名x</span><br></pre></td></tr></table></figure><h3 id="取消最近的一次add"><a href="#取消最近的一次add" class="headerlink" title="取消最近的一次add"></a>取消最近的一次add</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset &lt;file&gt;</span><br><span class="line">git reset</span><br></pre></td></tr></table></figure><h3 id="取消本地修改"><a href="#取消本地修改" class="headerlink" title="取消本地修改"></a>取消本地修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git checkout . <span class="comment">#本地所有修改的。没有的提交的，都返回到原来的状态</span></span><br><span class="line">git stash  <span class="comment">#把所有没有提交的修改暂存到stash里面。可用git stash pop回复。</span></span><br><span class="line">git reset --hard HASH <span class="comment">#返回到某个节点，不保留修改。</span></span><br><span class="line">git reset --soft HASH <span class="comment">#返回到某个节点。保留修改</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>anaconda-config</title>
      <link href="/2018/09/28/anaconda-config/"/>
      <url>/2018/09/28/anaconda-config/</url>
      
        <content type="html"><![CDATA[<h1 id="Install-anaconda-on-MacOS"><a href="#Install-anaconda-on-MacOS" class="headerlink" title="Install anaconda on MacOS"></a>Install anaconda on MacOS</h1><p><a href="https://mirrors.tuna.tsinghua.edu.cn/#" target="_blank" rel="noopener">清华镜像站</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3</span><br></pre></td></tr></table></figure><h1 id="Install-cv2"><a href="#Install-cv2" class="headerlink" title="Install cv2"></a>Install cv2</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c menpo opencv</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> anaconda python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pycaffe config </title>
      <link href="/2018/08/27/pycaffe-config/"/>
      <url>/2018/08/27/pycaffe-config/</url>
      
        <content type="html"><![CDATA[<h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>There is always some trouble when we want use <code>pycaffe</code> and <code>opencv</code> at the same time :(<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">import</span> cv2</span><br></pre></td></tr></table></figure></p><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><blockquote><p>We just do not use Anaconda!!!!!</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> caffe/python</span><br><span class="line"><span class="keyword">for</span> req <span class="keyword">in</span> $(cat requirements.txt); <span class="keyword">do</span> pip install <span class="variable">$req</span>; <span class="keyword">done</span></span><br><span class="line">pip install opencv-python</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_path</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</span><br><span class="line">        sys.path.insert(<span class="number">0</span>, path)</span><br><span class="line"></span><br><span class="line">caffe_path = <span class="string">'/home/zhaoxiandong/caffe'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add caffe to PYTHONPATH</span></span><br><span class="line">caffe_path = osp.join(caffe_path, <span class="string">'python'</span>)</span><br><span class="line">add_path(caffe_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># successful !::::))))</span></span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://github.com/NVIDIA/DIGITS/issues/156" target="_blank" rel="noopener">https://github.com/NVIDIA/DIGITS/issues/156</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> pycaffe </tag>
            
            <tag> caffe </tag>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PyTorch Begin</title>
      <link href="/2018/08/10/PyTorch-Begin/"/>
      <url>/2018/08/10/PyTorch-Begin/</url>
      
        <content type="html"><![CDATA[<h2 id="Recommand-approach-for-saving-model"><a href="#Recommand-approach-for-saving-model" class="headerlink" title="Recommand approach for saving model"></a>Recommand approach for saving model</h2><p><a href="https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch" target="_blank" rel="noopener">Stack overflow</a></p><ul><li><p>First</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = Model(args)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure></li><li><p>Second</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(mode, PATH)</span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = torch.load(PATH)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Pytorch-DataParallel"><a href="#Pytorch-DataParallel" class="headerlink" title="Pytorch DataParallel"></a>Pytorch DataParallel</h3><p><a href="https://blog.csdn.net/qq_19598705/article/details/80396325" target="_blank" rel="noopener">csdn</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ssh tunnel 端口转发</title>
      <link href="/2018/08/09/ssh-tunnel-%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"/>
      <url>/2018/08/09/ssh-tunnel-%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<h1 id="Problem-description"><a href="#Problem-description" class="headerlink" title="Problem description"></a>Problem description</h1><ul><li><strong>A</strong> PC</li><li><strong>B</strong> 有公网IP的服务器或者工作站</li><li><strong>C</strong> 和<strong>B</strong>在同一个局域网的机器</li><li><strong>D</strong> 任意一台能联网的机器</li></ul><p><img src="ssh_tunnel.png" alt="img"></p><p>我们想通过PC来连接<strong>B</strong>, <strong>C</strong>, <strong>D</strong>, 从而方便的来远程同步代码，和开启jupyter-notebook服务等。</p><h1 id="ssh-command"><a href="#ssh-command" class="headerlink" title="ssh command"></a>ssh command</h1><p>主要用到了下边这条命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L &lt;port2&gt;:&lt;ip1&gt;:&lt;port1&gt; &lt;username&gt;@&lt;ip&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>N</strong> 在后台运行</li><li><strong>f</strong> Fork into background after authentication. 后台认证用户密码，通常和-N连用，不用登录到远程主机。</li><li><strong>L</strong> 本地起端口映射到其他机器</li></ul><h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><h2 id="Access-server-C-on-PC-A"><a href="#Access-server-C-on-PC-A" class="headerlink" title="Access server C on PC A"></a>Access server C on PC A</h2><p>Run on PC A:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L &lt;A.custom.port&gt;:&lt;C.local.ip&gt;:&lt;C.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt;</span><br></pre></td></tr></table></figure></p><h2 id="Access-server-D-on-PC-A"><a href="#Access-server-D-on-PC-A" class="headerlink" title="Access server D on PC A"></a>Access server D on PC A</h2><p>Run on server D:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -CfnNt -R &lt;B.custom.port&gt;:localhost:&lt;D.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt;</span><br></pre></td></tr></table></figure></p><p>Run on PC A:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L &lt;A.custom.port&gt;:localhost:&lt;B.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt;</span><br></pre></td></tr></table></figure></p><h2 id="A直接ssh登陆到C"><a href="#A直接ssh登陆到C" class="headerlink" title="A直接ssh登陆到C"></a>A直接ssh登陆到C</h2><p>Add the following code to <code>~/.ssh/config</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host server</span><br><span class="line">User C.username</span><br><span class="line">Port 22</span><br><span class="line">HostName &lt;C.local.ip&gt;</span><br><span class="line">ProxyCommand ssh B.username@B.public.ip nc %h %p 2&gt; /dev/null</span><br></pre></td></tr></table></figure></p><p>Then, we can connect to server C directly.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh server</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://blog.creke.net/722.html" target="_blank" rel="noopener">梦溪博客</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> ssh tunnel </tag>
            
            <tag> sublime </tag>
            
            <tag> sftp </tag>
            
            <tag> jupyter-notebook </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python argparse</title>
      <link href="/2018/08/08/python-argparse/"/>
      <url>/2018/08/08/python-argparse/</url>
      
        <content type="html"><![CDATA[<h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser= argparse.ArgumentParser()</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">''</span>)</span><br><span class="line">parser.add_argument(<span class="string">'data'</span>, metavar=<span class="string">'DIR'</span>, help=<span class="string">'path to dataset'</span>)</span><br></pre></td></tr></table></figure><h2 id="Parameter"><a href="#Parameter" class="headerlink" title="Parameter"></a>Parameter</h2><ul><li>prog - The name of the program (default: sys.argv[0])</li><li>usage - The string describing the program usage (default: generated from arguments added to parser)</li><li>description - Text to display before the argument help (default: none)</li><li>epilog - Text to display after the argument help (default: none)</li><li>parents - A list of ArgumentParser objects whose arguments should also be included</li><li>formatter_class - A class for customizing the help output</li><li>prefix_chars - The set of characters that prefix optional arguments (default: ‘-‘)</li><li>fromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None)</li><li>argument_default - The global default value for arguments (default: None)</li><li>conflict_handler - The strategy for resolving conflicting optionals (usually unnecessary)</li><li>add_help - Add a -h/–help option to the parser (default: True)</li><li>allow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True)</li></ul><h2 id="The-add-augment-method"><a href="#The-add-augment-method" class="headerlink" title="The add_augment() method"></a>The add_augment() method</h2><ul><li>name or flags - Either a name or a list of option strings, e.g. foo or -f, –foo.</li><li>action - The basic type of action to be taken when this argument is encountered at the command line.</li><li>nargs - The number of command-line arguments that should be consumed.</li><li>const - A constant value required by some action and nargs selections.</li><li>default - The value produced if the argument is absent from the command line.</li><li>type - The type to which the command-line argument should be converted.</li><li>choices - A container of the allowable values for the argument.</li><li>required - Whether or not the command-line option may be omitted (optionals only).</li><li>help - A brief description of what the argument does.</li><li>metavar - A name for the argument in usage messages. 有助于提醒用户，该命令行参数所期待的参数，如 metavar=”mode”</li><li>dest - The name of the attribute to be added to the object returned by parse_args().</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://blog.csdn.net/itlance_ouyang/article/details/52489674" target="_blank" rel="noopener">Python 命令行解析</a><br><a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener">python.org</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> argparse </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
