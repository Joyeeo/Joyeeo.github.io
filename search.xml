<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[suvery-of-qcnn]]></title>
    <url>%2F2019%2F01%2F16%2Fsuvery-of-qcnn%2F</url>
    <content type="text"><![CDATA[Quantization AlgorithmsTruncation without re-training原理与分析将fp32的数，直接截断为8bit定点数以8bits为例，每层的权重共享一个QCode，每层的activation共享一个QCode。$$ x_q = \frac{clip(round(x_f\times 2^{qcode}), -2^7, 2^7-1)}{2^{qcode}}$$ 如何配置QCode accuracy-aware config对于分类网络来说，我们只关心它最后的分类精度，比如baseline网络在ImageNet上分类精度为70%，通过改变配置config，能够得到最后量化后的网络的分类精度。配置Qcode的指标只关心最后的分类精度。 Greedy layer-wise optimization另外，可以采用更加方便的方式，通过最小化$x_q$与$x$之间的KL散度或者MSE，来配置每层的QCode. 特点分析 运算简单，不需要重新进行训练 需要逐层配置QCode参数，耗费较多资源 QCode的量化编码方式过于简单，且Qcode只能取值为整数，对分类网络比较友好，但是对检测网络和其他类型的网络并不一定能轻松适配。 试验与结论使用这种方式进行离线量化，最后的分类网络的精度基本不会下降（千分位的变动）. DoReFa NetWRPN受到了Wide ResNet的启发，Wide ResNet减少网络的深度，扩展网络的宽度，通过重新设计了网络结构的方式来使得网络保持原有的精度。WRPN wide reduced-precision networks需要对weights和activation进行量化，相当于减少了网络的拟合能力，可以通过增加网络款宽度的方式来增加一部分参数来弥补量化带来的损失。 半成品的实现：Code 原理分析 将值限制在[-1, 1]之间 对w进行有符号量化，对a进行无符号量化$$ w_q = \frac{1}{2^{k-1}-1}round((2^{k-1}-1)\cdot w_f) \\ a_q = \frac{1}{2^{k}-1}round((2^{k}-1)\cdot a_f)$$ 特点分析 quantier的parameter是静态的，不需要像TTQ一样需要在训练中进行学习 In our work, we maintain the depth parameter same as baseline network but widen the filter maps. To be consistent with results reported in prior works, we do not quantize weights and activations of the first and last layer. 试验与结论 VNQ: Variational Network QuantizationOur method is an extension of Sparse VD 公式很多，还需要时间推导。 原理分析特点分析 The method does not require fine-tuning after quantization.可以从scratch开始训练，也可以直接使用预训练好的权重。 Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).只有小数据集小网络，小数据集大网络，没有大数据集和大网络，可能不具有普适性。 WAGE: Training and Inference with Integers in Deep Neural Networks官方源码：code,只包含cifar10的demo。将Weight,Activation,Gradient,Error都进行量化，量化方式采用$$ \sigma(k) = 2^{k-1}, k\in \mathbb{N_+} \\ Q(x, k) = clip(\sigma(k)\cdot round(\frac{x}{\sigma(k)}), -1+\sigma(k), 1-\sigma(k))$$ 试验与结论如果需要在AI芯片进行训练，WAGE是一个很好的研究方向；但是当前主流的做法是训练交给GPU，只需要在AI芯片上进行inference. Clip-Q: Deep network compression learning by In-Parallel Pruning Quantization We combine network pruning and weight quantization in a single learning framework 使用贝叶斯优化器来the pruning rate p and the bit budget b 只对weights进行了pruning和quantization. pruning是百分比的方式，quantization是聚类codebook的方式 试验与结论量化相关试验我认为还是比较相同bitwidth下的精度更切合实际应用，毕竟量化的作用一方面是压缩权重，更重要的还是AI芯片的inference加速，需要给出hardware friendly的分析与验证。 Bi-Real Net 较XOR-Net, BNN网络的分类accuracy有较大的提升 没有量化第一层和最后一层，并不Real:) 试验与结论 Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware ELQ: Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks 之前的方式大多是layer-wise 最小化量化前后数值的 Error，本文建立了quantization与loss之间的关系。其实在pruning中也有类似的改进，一般大家的认为weights越小越不重要，优先prune掉，后来提出fisher pruning就是根据loss来确定pruning的策略。前者简单直接，后者理论上能找到更优的解，但实现麻烦。 只量化了weights PACT: parameterized clipping activation for quantized neural networks PACT提出Activation的量化方法，并使用DoReFa量化weight，最终训练w4a4 网络在ImageNet上仍能达到很好的accuracy，编码方式也十分简单，很值得借鉴。 仍然没有量化第一层和最后一层 原理分析 Forward quantization$$ y = PACT(x) = 0.5(|x|-|x-\alpha|+\alpha) \\ y = round(y\cdot \frac{2^k-1}{\alpha})\cdot \frac{\alpha}{2^k-1}$$这里也看出k对应的是unsigned int的位数。 Backward, STE:$$ \cfrac{\partial y_q}{\partial\alpha} = \cfrac{\partial y_q}{\partial y}\cfrac{\partial y}{\partial\alpha}\simeq \cfrac{\partial y}{\partial \alpha}$$ 试验与结论weights使用DoReFa方式量化，activations使用PACT方式量化，可以看到在ResNet18上w4a4只和baseline低了一个点。 References: paper-of-quantization nervanasystems.github.io]]></content>
      <tags>
        <tag>quantization</tag>
        <tag>CNN</tag>
        <tag>compression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper-of-quantization]]></title>
    <url>%2F2019%2F01%2F11%2Fpaper-of-quantization%2F</url>
    <content type="text"><![CDATA[Related papers Deep learning with limited numerical precision. 2015 IBM DoReFa-Net: Training low bit-width convolutional neural networks with low bit-width gradients. 2016 XNOR-Net: ImageNet Classification using binary convolutional neural networks. 2016 washington BC: Binary connect. BNN: Binarized Neural Networks. NIPS2016 Fixed point quantization of deep convolutional networks. 2016 Hardware-oriented approximation of convolutional neural networks. ICLR2016 TWNs: Ternary weight networks. NIPS2016 ucas Quantized convolutional neural networks for mobile devices. CVPR2016 nlpr Flexpoint: an adaptive numerical format for efficient training of deep neural networks. 2017 intel INQ: Incremental network quantization, towards lossless CNNs with low-precision weights. ICLR2017 intel labs china TTQ: Trained ternary quantization. ICLR2017 stanford WRPN: wide reduced-precision networks. 2017 detailed A Survey of Model Compression and Acceleration for Deep Neural Networks. 201712 VNQ: Variational network quantization. ICLR2018 WAGE: Training and Inference with Integers in Deep Neural Networks. ICLR2018 oral tsinghua Clip-Q: Deep network compression learning by In-Parallel Pruning Quantization. CVPR2018 SFU LQ-NETs: learned quantization for highly accurate and compact deep neural networks. ECCV2018 Microsoft Bi-Real Net: Enhancing the performance of 1-bit CNNs with improved Representational capability and advanced training algorithm. ECCV2018 HKU Synergy: Algorithm-hardware co-design for convnet accelerators on embedded FPGAs. 2018 UC Berkeley Alternating multi-bit quantization for recurrent neural networks. ICLR2018 alibaba Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware. 2018 ELQ: Explicit loss-error-aware quantization for low-bit deep neural networks. CVPR2018 intel tsinghua From Hashing to CNNs: training Binary weights vis hashing. AAAI2018 nlpr HAQ: Hardware-Aware automated quantization. NIPS workshop2018 mit Heterogeneous Bitwidth Binarization in Convolutional Neural Networks. NIPS2018 microsoft HALP: High-Accuracy Low-Precision Training. 2018 stanford Mixed Precision Training. ICLR2018 baidu PACT: parameterized clipping activation for quantized neural networks. 2018 IBM Model Compression via distillation and quantization. ICLR2018 google Quantization and training of neural networks for efficient integer-arithmetic-only inference. CVPR2018 Google Quantized back-propagation: training binarized neural networks with quantized gradients. ICLR2018 QUENN: Quantization engine for low-power neural networks. CF18 ACM Scalable methods for 8-bits training of neural networks. NIPS2018 intel SYQ: learning symmetric quantization for efficient deep neural networks. CVPR2018 xilinx TSQ: two-step quantization for low-bit neural networks. CVPR2018 V-Quant: Value-aware quantization for training and inference of neural networks. ECCV2018 facebook UNIQ: Uniform noise injection for non-uniform quantization of neural networks. 2018 Training a binary weight object detector by knowledge transfer for autonomous driving. 2018 Training competitive binary neural networks from scratch. 2018 A white-paper: Quantizing deep convolutional networks for efficient inference. 2018 google ACIQ: analytical clipping for integer quantization of neural networks. ICLR2019 Intel Per-Tensor Fixed-point quantization of the back-propagation algorithm. ICLR2019]]></content>
      <tags>
        <tag>paper</tag>
        <tag>quantization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mxnet-build-from-source]]></title>
    <url>%2F2019%2F01%2F08%2Fmxnet-build-from-source%2F</url>
    <content type="text"><![CDATA[Build MXNet from SourceClone the MXNet Project12git clone --recursive https://github.com/apache/incubator-mxnet mxnetcd mxnet Download mklcudnn12cd 3rdparty/mkldnn/externalwget https://github.com/intel/mkl-dnn/releases/download/v0.17.2/mklml_lnx_2019.0.1.20180928.tgz Build12cd docs/install ./install_mxnet_ubuntu_python.sh install python12cd pythonpip install -e . Add operator in backendWhy not add custom operator using PythonOp interface.123456789class NDArrayOp(PythonOp): """Base class for numpy operators. numpy operators allow parts of computation in symbolic graph to be writen in numpy. This feature is intended for quickly hacking out a solution for non performance critical parts. Please consider write a c++ implementation if it becomes a bottleneck. Note that if your operator contains internal states (like arrays), it cannot be used for multi-gpu training. """ Referencesinstall-mxnet-for-pythonadd op in backend]]></content>
      <tags>
        <tag>MXNet</tag>
        <tag>GLUON</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install-tensorRT]]></title>
    <url>%2F2018%2F10%2F26%2Finstall-tensorRT%2F</url>
    <content type="text"><![CDATA[Install Driver and CUDAinstall cuda Install TensorRTDownload TensorRT 5.0tensorrt download Then, I got the following package:nv-tensorrt-repo-ubuntu1604-cuda9.0-trt5.0.0.10-rc-20180906_1-1_amd64.deb Install TensorRT 5.01234567sudo dpkg -i nv-tensorrt-repo-ubuntu1604-cuda9.0-trt5.0.0.10-rc-20180906_1-1_amd64.debsudo apt-key add /var/nv-tensorrt-repo-cuda9.0-trt5.0.0.10-rc-20180906/7fa2af80.pub sudo apt-get update# Unluckily, I encountered the following problem.## double free or corruption (fasttop): 0x0000000001368e00 ***# I solved it by run: sudo apt-get purge libappstream3 sudo apt-get install tensorrt Demo1234cd /usr/src/tensorrt/samplessudo make -j32cd ../bin./samples_mnist Install PyCUDA12345678pip install pycuda# error# In file included from src/cpp/cuda.cpp:1:0:# src/cpp/cuda.hpp:14:18: fatal error: cuda.h: No such file or directory# compilation terminated.# error: command 'gcc' failed with exit status 1export PATH=/usr/local/cuda/bin:$PATHpip install pycuda uff custom plugin123456cd /usr/src/tensorrt/samples/python/uff_custom_pluginmkdir build &amp;&amp; pushd buildcmake ..make -j8python2 lenet5.pypython2 mnist_uff_custom_plugin.py]]></content>
      <tags>
        <tag>cuda</tag>
        <tag>nvidia-driver</tag>
        <tag>ubuntu</tag>
        <tag>tensorRT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install-cuda]]></title>
    <url>%2F2018%2F10%2F18%2Finstall-cuda%2F</url>
    <content type="text"><![CDATA[Update /etc/apt/source.listBase software12345678sudo apt-get install vimsudo apt-get install screensudo apt-get install gitsudo apt-get install zshsh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)"sudo apt-get install htopsudo apt-get install graphvizsudo apt-get install unrar CUDA9.01234567891011121314cd /home/ict/Downloads/cuda9# nvidia-driversudo dpkg -i nvidia-driver-local-repo-ubuntu1604-387.34_1.0-1_amd64.debsudo apt-get updatesudo apt-get install cuda-driverssudo reboot# cuda9.0cd /home/ict/Downloads/cuda9sudo dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-debsudo apt-get updatesudo apt-get install cuda# cudnnsudo dpkg -i libcudnn7-dev_7.1.3.16-1+cuda9.0_amd64.debsudo dpkg -i libcudnn7_7.1.3.16-1+cuda9.0_amd64.deb Remove CUDA1sudo apt-get --purge remove "nvidia-*" Using the specified GPU1CUDA_VISIBLE_DEVICES="0,1"]]></content>
      <tags>
        <tag>cuda</tag>
        <tag>nvidia-driver</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git-command]]></title>
    <url>%2F2018%2F09%2F29%2Fgit-command%2F</url>
    <content type="text"><![CDATA[Git commandupdate a forked repo from remote repo.123456789git remote add upstream git@github.com:&lt;custom&gt;.gitgit remote -vgit fetch upstreamgit merge upstream/mastergit push git 合并多个commits合并多个 Commit git拉取远程分支到本地查看远程分支1git branch -r 拉取远程分支到本地分支1git checkout -b 本地分支名x origin/远程分支名x 取消本地修改1234git checkout . #本地所有修改的。没有的提交的，都返回到原来的状态git stash #把所有没有提交的修改暂存到stash里面。可用git stash pop回复。git reset --hard HASH #返回到某个节点，不保留修改。git reset --soft HASH #返回到某个节点。保留修改]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[anaconda-config]]></title>
    <url>%2F2018%2F09%2F28%2Fanaconda-config%2F</url>
    <content type="text"><![CDATA[Install anaconda on MacOS清华镜像站 1bash Anaconda3 Install cv21conda install -c menpo opencv]]></content>
      <tags>
        <tag>anaconda python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycaffe config]]></title>
    <url>%2F2018%2F08%2F27%2Fpycaffe-config%2F</url>
    <content type="text"><![CDATA[ProblemThere is always some trouble when we want use pycaffe and opencv at the same time :(12import caffeimport cv2 Solution We just do not use Anaconda!!!!! 123cd caffe/pythonfor req in $(cat requirements.txt); do pip install $req; donepip install opencv-python 12345678910111213141516import os.path as ospimport sysdef add_path(path): if path not in sys.path: sys.path.insert(0, path)caffe_path = '/home/zhaoxiandong/caffe'# Add caffe to PYTHONPATHcaffe_path = osp.join(caffe_path, 'python')add_path(caffe_path)import caffeimport cv2# successful !::::)))) Referenceshttps://github.com/NVIDIA/DIGITS/issues/156]]></content>
      <tags>
        <tag>pycaffe</tag>
        <tag>caffe</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch Begin]]></title>
    <url>%2F2018%2F08%2F10%2FPyTorch-Begin%2F</url>
    <content type="text"><![CDATA[Recommand approach for saving modelStack overflow First 12345# savetorch.save(model.state_dict(), PATH)# loadmodel = Model(args)model.load_state_dict(torch.load(PATH)) Second 1234# savetorch.save(mode, PATH)# loadmodel = torch.load(PATH) Pytorch DataParallelcsdn]]></content>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh tunnel 端口转发]]></title>
    <url>%2F2018%2F08%2F09%2Fssh-tunnel-%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%2F</url>
    <content type="text"><![CDATA[Problem description A PC B 有公网IP的服务器或者工作站 C 和B在同一个局域网的机器 D 任意一台能联网的机器 我们想通过PC来连接B, C, D, 从而方便的来远程同步代码，和开启jupyter-notebook服务等。 ssh command主要用到了下边这条命令： 1ssh -N -f -L &lt;port2&gt;:&lt;ip1&gt;:&lt;port1&gt; &lt;username&gt;@&lt;ip&gt; N 在后台运行 f Fork into background after authentication. 后台认证用户密码，通常和-N连用，不用登录到远程主机。 L 本地起端口映射到其他机器 ExampleAccess server C on PC ARun on PC A:1ssh -N -f -L &lt;A.custom.port&gt;:&lt;C.local.ip&gt;:&lt;C.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt; Access server D on PC ARun on server D:1ssh -CfnNt -R &lt;B.custom.port&gt;:localhost:&lt;D.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt; Run on PC A:1ssh -N -f -L &lt;A.custom.port&gt;:localhost:&lt;B.custom.port&gt; &lt;username&gt;@&lt;B.public.ip&gt; A直接ssh登陆到CAdd the following code to ~/.ssh/config12345Host serverUser C.usernamePort 22HostName &lt;C.local.ip&gt;ProxyCommand ssh B.username@B.public.ip nc %h %p 2&gt; /dev/null Then, we can connect to server C directly.1ssh server Reference梦溪博客]]></content>
      <tags>
        <tag>ssh tunnel</tag>
        <tag>sublime</tag>
        <tag>sftp</tag>
        <tag>jupyter-notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python argparse]]></title>
    <url>%2F2018%2F08%2F08%2Fpython-argparse%2F</url>
    <content type="text"><![CDATA[Usage1234import argparseparser= argparse.ArgumentParser()parser = argparse.ArgumentParser(description='')parser.add_argument('data', metavar='DIR', help='path to dataset') Parameter prog - The name of the program (default: sys.argv[0]) usage - The string describing the program usage (default: generated from arguments added to parser) description - Text to display before the argument help (default: none) epilog - Text to display after the argument help (default: none) parents - A list of ArgumentParser objects whose arguments should also be included formatter_class - A class for customizing the help output prefix_chars - The set of characters that prefix optional arguments (default: ‘-‘) fromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None) argument_default - The global default value for arguments (default: None) conflict_handler - The strategy for resolving conflicting optionals (usually unnecessary) add_help - Add a -h/–help option to the parser (default: True) allow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True) The add_augment() method name or flags - Either a name or a list of option strings, e.g. foo or -f, –foo. action - The basic type of action to be taken when this argument is encountered at the command line. nargs - The number of command-line arguments that should be consumed. const - A constant value required by some action and nargs selections. default - The value produced if the argument is absent from the command line. type - The type to which the command-line argument should be converted. choices - A container of the allowable values for the argument. required - Whether or not the command-line option may be omitted (optionals only). help - A brief description of what the argument does. metavar - A name for the argument in usage messages. 有助于提醒用户，该命令行参数所期待的参数，如 metavar=”mode” dest - The name of the attribute to be added to the object returned by parse_args(). ReferencePython 命令行解析python.org]]></content>
      <tags>
        <tag>python</tag>
        <tag>argparse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F08%2F07%2Ftest%2F</url>
    <content type="text"><![CDATA[test $$\begin{eqnarray}\nabla\cdot\vec{E} &amp;=&amp; \frac{\rho}{\epsilon_0} \\\nabla\cdot\vec{B} &amp;=&amp; 0 \\\nabla\times\vec{E} &amp;=&amp; -\frac{\partial B}{\partial t} \\\nabla\times\vec{B} &amp;=&amp; \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)\end{eqnarray}$$]]></content>
  </entry>
</search>
